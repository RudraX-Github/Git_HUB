{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df69de8",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Engineering for Commodity Price Predictor\n",
    "\n",
    "## Objective\n",
    "Engineer 100-150 features from price data and weather data for ML model training.\n",
    "\n",
    "**Key Focus**: Temperature lag features (correlation +0.81 from Phase 1)\n",
    "\n",
    "## Expected Output\n",
    "- Engineered dataset with 100-150 features\n",
    "- 13,900 rows Ã— 100-150 columns\n",
    "- ML-ready normalized data\n",
    "- Feature documentation\n",
    "\n",
    "## Timeline\n",
    "Estimated: 7-12 hours for all 10 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b81cc1",
   "metadata": {},
   "source": [
    "## STEP 1: Environment Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96fe1389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n",
      "   Pandas version: 2.2.3\n",
      "   NumPy version: 2.3.4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"   Pandas version: {pd.__version__}\")\n",
    "print(f\"   NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879c04f8",
   "metadata": {},
   "source": [
    "### Load Price Data from Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d1e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Price data loaded\n",
      "   Records: 14,965\n",
      "   Columns: 18\n",
      "   Date Range: 2024-01-01 to 2025-01-01\n",
      "   Missing Values: 0\n",
      "\n",
      "   Columns: ['State', 'District', 'Market', 'Commodity', 'Variety', 'Grade', 'Arrival_Date', 'Min_Price', 'Max_Price', 'Modal_Price', 'Commodity_Code', 'Year', 'Month', 'Day', 'DayName', 'Quarter', 'Price_Range', 'Price_Volatility_%']\n"
     ]
    }
   ],
   "source": [
    "# Load processed price data\n",
    "processed_data_dir = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Processed_Data\"\n",
    "price_file = os.path.join(processed_data_dir, 'Price_Data_Processed.csv')\n",
    "\n",
    "df_prices = pd.read_csv(price_file)\n",
    "df_prices['Arrival_Date'] = pd.to_datetime(df_prices['Arrival_Date'])\n",
    "\n",
    "print(f\"âœ… Price data loaded\")\n",
    "print(f\"   Records: {len(df_prices):,}\")\n",
    "print(f\"   Columns: {len(df_prices.columns)}\")\n",
    "print(f\"   Date Range: {df_prices['Arrival_Date'].min().date()} to {df_prices['Arrival_Date'].max().date()}\")\n",
    "print(f\"   Missing Values: {df_prices.isnull().sum().sum()}\")\n",
    "print(f\"\\n   Columns: {list(df_prices.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cdcfb9",
   "metadata": {},
   "source": [
    "### Load Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a06bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Weather data loaded\n",
      "   Records: 7,707\n",
      "   Date Range: 2024-01-01 to 2025-01-01\n",
      "\n",
      "   Weather Columns: ['market_name', 'latitude', 'longitude', 'time', 'temperature_max', 'temperature_min', 'temperature_mean', 'precipitation_sum', 'rain_sum', 'wind_speed_max']\n"
     ]
    }
   ],
   "source": [
    "# Connect to weather database\n",
    "DB_HOST = os.environ.get('DB_HOST', 'localhost')\n",
    "DB_USER = os.environ.get('DB_USER', 'root')\n",
    "DB_PASS = os.environ.get('DB_PASS', 'root')\n",
    "DB_NAME = os.environ.get('DB_NAME', 'weather_history')\n",
    "\n",
    "try:\n",
    "    conn = mysql.connector.connect(\n",
    "        host=DB_HOST,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS,\n",
    "        database=DB_NAME\n",
    "    )\n",
    "    df_weather = pd.read_sql(\"SELECT * FROM cleaned_weather_data;\", conn)\n",
    "    conn.close()\n",
    "    \n",
    "    df_weather.columns = [c.strip() for c in df_weather.columns]\n",
    "    if 'time' in df_weather.columns:\n",
    "        df_weather['date'] = pd.to_datetime(df_weather['time'])\n",
    "    \n",
    "    print(f\"âœ… Weather data loaded\")\n",
    "    print(f\"   Records: {len(df_weather):,}\")\n",
    "    print(f\"   Date Range: {df_weather['date'].min().date()} to {df_weather['date'].max().date()}\")\n",
    "    print(f\"\\n   Weather Columns: {list(df_weather.columns[:10])}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load weather from DB: {e}\")\n",
    "    df_weather = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315aac7e",
   "metadata": {},
   "source": [
    "## STEP 2: Data Merge & Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8ef27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data prepared for merging\n",
      "   Unique Commodity-Market pairs: 129\n",
      "âœ… Data merged successfully\n",
      "   Merged records: 14,965\n",
      "   Merge success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Prepare price data for merging\n",
    "df_prices['date'] = df_prices['Arrival_Date'].dt.date\n",
    "\n",
    "# Create commodity-market groups\n",
    "df_prices['Commodity_Market'] = df_prices['Commodity'] + '_' + df_prices['Market']\n",
    "\n",
    "print(f\"âœ… Data prepared for merging\")\n",
    "print(f\"   Unique Commodity-Market pairs: {df_prices['Commodity_Market'].nunique()}\")\n",
    "\n",
    "# Merge with weather data\n",
    "if not df_weather.empty:\n",
    "    df_weather['date'] = pd.to_datetime(df_weather['date']).dt.date\n",
    "    \n",
    "    # Create market mapping if available\n",
    "    # For now, aggregate weather by date only\n",
    "    df_weather_daily = df_weather.groupby('date').agg({\n",
    "        'temperature_max': 'mean',\n",
    "        'temperature_min': 'mean',\n",
    "        'temperature_mean': 'mean',\n",
    "        'precipitation_sum': 'mean',\n",
    "        'rain_sum': 'mean',\n",
    "        'wind_speed_max': 'mean',\n",
    "        'wind_gusts_max': 'mean',\n",
    "        'cloud_cover': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Merge price and weather\n",
    "    df_combined = df_prices.merge(\n",
    "        df_weather_daily,\n",
    "        on='date',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Data merged successfully\")\n",
    "    print(f\"   Merged records: {len(df_combined):,}\")\n",
    "    print(f\"   Merge success rate: {len(df_combined[df_combined['temperature_min'].notna()]) / len(df_combined) * 100:.1f}%\")\n",
    "else:\n",
    "    df_combined = df_prices.copy()\n",
    "    print(f\"âš ï¸  Weather data unavailable, proceeding with price data only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6987508c",
   "metadata": {},
   "source": [
    "## STEP 3: Lagged Price Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54fa9f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Lagged price features created\n",
      "   Features added: 12\n",
      "   Example features: ['Price_Lag_1', 'Min_Price_Lag_1', 'Max_Price_Lag_1']\n",
      "   NaN values in lags: 15,972 (first 30 rows expected)\n"
     ]
    }
   ],
   "source": [
    "# Sort by commodity-market and date\n",
    "df_combined = df_combined.sort_values(['Commodity_Market', 'Arrival_Date']).reset_index(drop=True)\n",
    "\n",
    "# Create lagged price features\n",
    "for lag in [1, 7, 14, 30]:\n",
    "    df_combined[f'Price_Lag_{lag}'] = df_combined.groupby('Commodity_Market')['Modal_Price'].shift(lag)\n",
    "    df_combined[f'Min_Price_Lag_{lag}'] = df_combined.groupby('Commodity_Market')['Min_Price'].shift(lag)\n",
    "    df_combined[f'Max_Price_Lag_{lag}'] = df_combined.groupby('Commodity_Market')['Max_Price'].shift(lag)\n",
    "\n",
    "print(f\"âœ… Lagged price features created\")\n",
    "lag_features = [col for col in df_combined.columns if 'Lag' in col]\n",
    "print(f\"   Features added: {len(lag_features)}\")\n",
    "print(f\"   Example features: {lag_features[:3]}\")\n",
    "print(f\"   NaN values in lags: {df_combined[lag_features].isnull().sum().sum():,} (first 30 rows expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da2c73",
   "metadata": {},
   "source": [
    "## STEP 4: Rolling Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f082fe40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Rolling statistics created\n",
      "   Features added: 22\n",
      "   Example features: ['Min_Price', 'Max_Price', 'Min_Price_Lag_1']\n"
     ]
    }
   ],
   "source": [
    "# Rolling averages and statistics\n",
    "for window in [7, 14, 30]:\n",
    "    df_combined[f'Price_MA_{window}'] = df_combined.groupby('Commodity_Market')['Modal_Price'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).mean()\n",
    "    )\n",
    "    df_combined[f'Price_Std_{window}'] = df_combined.groupby('Commodity_Market')['Modal_Price'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).std()\n",
    "    )\n",
    "    df_combined[f'Price_Max_{window}'] = df_combined.groupby('Commodity_Market')['Modal_Price'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).max()\n",
    "    )\n",
    "    df_combined[f'Price_Min_{window}'] = df_combined.groupby('Commodity_Market')['Modal_Price'].transform(\n",
    "        lambda x: x.rolling(window, min_periods=1).min()\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Rolling statistics created\")\n",
    "rolling_features = [col for col in df_combined.columns if 'MA_' in col or 'Std_' in col or ('Max_' in col and 'Price' in col) or ('Min_' in col and 'Price' in col)]\n",
    "print(f\"   Features added: {len(rolling_features)}\")\n",
    "print(f\"   Example features: {rolling_features[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94de22",
   "metadata": {},
   "source": [
    "## STEP 5: Momentum & Trend Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b8a897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Momentum features created\n",
      "   Features added: 4\n",
      "   Example: ['Price_Change_1d_%', 'Price_Change_7d_%']\n",
      "   NaN count: 3,821\n"
     ]
    }
   ],
   "source": [
    "# Percentage changes\n",
    "df_combined['Price_Change_1d_%'] = df_combined.groupby('Commodity_Market')['Modal_Price'].pct_change() * 100\n",
    "df_combined['Price_Change_7d_%'] = df_combined.groupby('Commodity_Market')['Modal_Price'].pct_change(7) * 100\n",
    "df_combined['Price_Change_30d_%'] = df_combined.groupby('Commodity_Market')['Modal_Price'].pct_change(30) * 100\n",
    "\n",
    "# Price range (already exists but create normalized version)\n",
    "df_combined['Price_Range_Normalized'] = (\n",
    "    (df_combined['Max_Price'] - df_combined['Min_Price']) / df_combined['Min_Price'].replace(0, 1) * 100\n",
    ")\n",
    "\n",
    "print(f\"âœ… Momentum features created\")\n",
    "momentum_features = [col for col in df_combined.columns if 'Change' in col or 'Range_Normalized' in col]\n",
    "print(f\"   Features added: {len(momentum_features)}\")\n",
    "print(f\"   Example: {momentum_features[:2]}\")\n",
    "print(f\"   NaN count: {df_combined[momentum_features].isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0075b",
   "metadata": {},
   "source": [
    "## STEP 6: Calendar & Seasonal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806b8110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Calendar & seasonal features created\n",
      "   Features added: 9\n",
      "   Example features: ['Day_of_Week', 'Day_of_Year', 'Week_of_Year', 'Is_Weekend', 'Season']\n"
     ]
    }
   ],
   "source": [
    "# Extract temporal features (already present, but add more)\n",
    "df_combined['Day_of_Week'] = df_combined['Arrival_Date'].dt.dayofweek\n",
    "df_combined['Day_of_Year'] = df_combined['Arrival_Date'].dt.dayofyear\n",
    "df_combined['Week_of_Year'] = df_combined['Arrival_Date'].dt.isocalendar().week\n",
    "df_combined['Is_Weekend'] = (df_combined['Day_of_Week'] >= 5).astype(int)\n",
    "\n",
    "# Seasonal indicators (India agricultural seasons)\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 1  # Winter\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 2  # Summer\n",
    "    elif month in [6, 7, 8, 9]:\n",
    "        return 3  # Monsoon\n",
    "    else:\n",
    "        return 4  # Post-Monsoon\n",
    "\n",
    "df_combined['Season'] = df_combined['Month'].apply(get_season)\n",
    "\n",
    "# Cyclical encoding (sine/cosine for circular features)\n",
    "df_combined['Month_Sin'] = np.sin(2 * np.pi * df_combined['Month'] / 12)\n",
    "df_combined['Month_Cos'] = np.cos(2 * np.pi * df_combined['Month'] / 12)\n",
    "df_combined['DayOfWeek_Sin'] = np.sin(2 * np.pi * df_combined['Day_of_Week'] / 7)\n",
    "df_combined['DayOfWeek_Cos'] = np.cos(2 * np.pi * df_combined['Day_of_Week'] / 7)\n",
    "\n",
    "print(f\"âœ… Calendar & seasonal features created\")\n",
    "seasonal_features = [col for col in df_combined.columns if 'Day_of' in col or 'Week' in col or 'Weekend' in col or 'Season' in col or 'Sin' in col or 'Cos' in col]\n",
    "print(f\"   Features added: {len(seasonal_features)}\")\n",
    "print(f\"   Example features: {seasonal_features[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d06d325",
   "metadata": {},
   "source": [
    "## STEP 7: Weather Features & Lags (CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f6c321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Creating weather features (Temperature Correlation: +0.81)\n",
      "   Weather features added: 31\n",
      "   Example features: ['Temp_Min', 'Temp_Max', 'Temp_Mean', 'Temp_Min_Lag_1', 'Temp_Max_Lag_1']\n"
     ]
    }
   ],
   "source": [
    "if not df_weather.empty and 'temperature_min' in df_combined.columns:\n",
    "    print(f\"âœ… Creating weather features (Temperature Correlation: +0.81)\")\n",
    "    \n",
    "    # Temperature features with lags (CRITICAL - correlation +0.81)\n",
    "    for lag in [0, 1, 3, 7]:\n",
    "        if lag == 0:\n",
    "            df_combined['Temp_Min'] = df_combined['temperature_min']\n",
    "            df_combined['Temp_Max'] = df_combined['temperature_max']\n",
    "            df_combined['Temp_Mean'] = df_combined['temperature_mean']\n",
    "        else:\n",
    "            df_combined[f'Temp_Min_Lag_{lag}'] = df_combined['temperature_min'].shift(lag)\n",
    "            df_combined[f'Temp_Max_Lag_{lag}'] = df_combined['temperature_max'].shift(lag)\n",
    "            df_combined[f'Temp_Mean_Lag_{lag}'] = df_combined['temperature_mean'].shift(lag)\n",
    "    \n",
    "    # Temperature rolling statistics\n",
    "    for window in [7, 14, 30]:\n",
    "        df_combined[f'Temp_Min_MA_{window}'] = df_combined['temperature_min'].rolling(window, min_periods=1).mean()\n",
    "        df_combined[f'Temp_Max_MA_{window}'] = df_combined['temperature_max'].rolling(window, min_periods=1).mean()\n",
    "        df_combined[f'Temp_Range_{window}'] = (\n",
    "            df_combined['temperature_max'].rolling(window, min_periods=1).max() -\n",
    "            df_combined['temperature_min'].rolling(window, min_periods=1).min()\n",
    "        )\n",
    "    \n",
    "    # Precipitation features\n",
    "    if 'precipitation_sum' in df_combined.columns:\n",
    "        df_combined['Rainfall'] = df_combined['precipitation_sum']\n",
    "        df_combined['Is_Rainy'] = (df_combined['precipitation_sum'] > 1).astype(int)\n",
    "        df_combined['Rainfall_Lag_1'] = df_combined['precipitation_sum'].shift(1)\n",
    "        df_combined['Rainfall_Lag_7'] = df_combined['precipitation_sum'].shift(7)\n",
    "        \n",
    "        for window in [7, 30]:\n",
    "            df_combined[f'Rainfall_Sum_{window}'] = df_combined['precipitation_sum'].rolling(window, min_periods=1).sum()\n",
    "    \n",
    "    # Wind features\n",
    "    if 'wind_speed_max' in df_combined.columns:\n",
    "        df_combined['Wind_Speed'] = df_combined['wind_speed_max']\n",
    "        df_combined['Wind_Speed_Lag_1'] = df_combined['wind_speed_max'].shift(1)\n",
    "        df_combined['Wind_Speed_MA_7'] = df_combined['wind_speed_max'].rolling(7, min_periods=1).mean()\n",
    "    \n",
    "    # Cloud cover\n",
    "    if 'cloud_cover' in df_combined.columns:\n",
    "        df_combined['Cloud_Cover'] = df_combined['cloud_cover']\n",
    "        df_combined['Cloud_Cover_Lag_1'] = df_combined['cloud_cover'].shift(1)\n",
    "    \n",
    "    weather_features = [col for col in df_combined.columns if 'Temp' in col or 'Wind' in col or 'Cloud' in col or 'Rainfall' in col]\n",
    "    print(f\"   Weather features added: {len(weather_features)}\")\n",
    "    print(f\"   Example features: {weather_features[:5]}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Weather data unavailable for feature engineering\")\n",
    "    weather_features = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17888e",
   "metadata": {},
   "source": [
    "## STEP 8: Commodity-Market Business Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2610de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Business features created\n",
      "   Features added: 6\n",
      "   Example features: ['Commodity_Avg_Price', 'Market_Avg_Price', 'Comm_Market_Avg_Price', 'Comm_Market_Std_Price', 'Comm_Market_Records']\n"
     ]
    }
   ],
   "source": [
    "# Historical commodity statistics\n",
    "commodity_stats = df_combined.groupby('Commodity').agg({\n",
    "    'Modal_Price': ['mean', 'std', 'min', 'max'],\n",
    "    'Price_Volatility_%': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "commodity_stats.columns = ['Commodity', 'Commodity_Avg_Price', 'Commodity_Std_Price',\n",
    "                           'Commodity_Min_Price', 'Commodity_Max_Price', 'Commodity_Volatility']\n",
    "\n",
    "df_combined = df_combined.merge(commodity_stats, on='Commodity', how='left')\n",
    "\n",
    "# Historical market statistics\n",
    "market_stats = df_combined.groupby('Market').agg({\n",
    "    'Modal_Price': ['mean', 'std'],\n",
    "    'Commodity': 'nunique'\n",
    "}).reset_index()\n",
    "\n",
    "market_stats.columns = ['Market', 'Market_Avg_Price', 'Market_Std_Price', 'Market_Commodity_Count']\n",
    "df_combined = df_combined.merge(market_stats, on='Market', how='left')\n",
    "\n",
    "# Commodity-Market interaction\n",
    "comm_market_stats = df_combined.groupby(['Commodity', 'Market']).agg({\n",
    "    'Modal_Price': ['mean', 'std', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "comm_market_stats.columns = ['Commodity', 'Market', 'Comm_Market_Avg_Price',\n",
    "                             'Comm_Market_Std_Price', 'Comm_Market_Records']\n",
    "\n",
    "df_combined = df_combined.merge(comm_market_stats, on=['Commodity', 'Market'], how='left')\n",
    "\n",
    "# Seasonal commodity premium\n",
    "seasonal_premium = df_combined.groupby(['Month', 'Commodity'])['Modal_Price'].mean().reset_index()\n",
    "seasonal_premium.columns = ['Month', 'Commodity', 'Month_Commodity_Avg_Price']\n",
    "df_combined = df_combined.merge(seasonal_premium, on=['Month', 'Commodity'], how='left')\n",
    "\n",
    "print(f\"âœ… Business features created\")\n",
    "business_features = [col for col in df_combined.columns if any(x in col for x in ['Commodity_Avg', 'Market_Avg', 'Comm_Market', 'Month_Commodity'])]\n",
    "print(f\"   Features added: {len(business_features)}\")\n",
    "print(f\"   Example features: {business_features[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e271b",
   "metadata": {},
   "source": [
    "## STEP 9: Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10cb88de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Categorical encoding completed\n",
      "   Encoding features added: 12\n",
      "   Example features: ['Day_of_Week', 'Day_of_Year', 'Grade_Medium', 'Grade_Small', 'Day_Monday']\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding for Grade\n",
    "df_combined = pd.get_dummies(df_combined, columns=['Grade'], drop_first=True, prefix='Grade')\n",
    "\n",
    "# One-hot encoding for DayName\n",
    "if 'DayName' in df_combined.columns:\n",
    "    df_combined = pd.get_dummies(df_combined, columns=['DayName'], drop_first=True, prefix='Day')\n",
    "\n",
    "# Target encoding for Commodity\n",
    "commodity_encoding = df_combined.groupby('Commodity')['Modal_Price'].mean()\n",
    "df_combined['Commodity_Encoded'] = df_combined['Commodity'].map(commodity_encoding)\n",
    "\n",
    "# Target encoding for Market\n",
    "market_encoding = df_combined.groupby('Market')['Modal_Price'].mean()\n",
    "df_combined['Market_Encoded'] = df_combined['Market'].map(market_encoding)\n",
    "\n",
    "print(f\"âœ… Categorical encoding completed\")\n",
    "encoding_features = [col for col in df_combined.columns if 'Grade_' in col or 'Day_' in col or 'Encoded' in col]\n",
    "print(f\"   Encoding features added: {len(encoding_features)}\")\n",
    "print(f\"   Example features: {encoding_features[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a05e41",
   "metadata": {},
   "source": [
    "## STEP 10: Data Validation & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9b20750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset before dropping NaN rows: 14,935\n",
      "Dataset after dropping first 30 rows: 14,905\n",
      "\n",
      "âœ… No remaining NaN values\n",
      "\n",
      "âœ… Data cleaned\n",
      "   Final shape: (14905, 117)\n",
      "   Remaining NaN: 0\n"
     ]
    }
   ],
   "source": [
    "# Drop first 30 rows (lag values not available)\n",
    "print(f\"Dataset before dropping NaN rows: {len(df_combined):,}\")\n",
    "df_combined = df_combined.iloc[30:].reset_index(drop=True)\n",
    "print(f\"Dataset after dropping first 30 rows: {len(df_combined):,}\")\n",
    "\n",
    "# Handle remaining NaNs\n",
    "nan_counts = df_combined.isnull().sum()\n",
    "nan_cols = nan_counts[nan_counts > 0]\n",
    "\n",
    "if len(nan_cols) > 0:\n",
    "    print(f\"\\nColumns with NaN values:\")\n",
    "    for col, count in nan_cols.items():\n",
    "        print(f\"  {col}: {count:,} ({count/len(df_combined)*100:.1f}%)\")\n",
    "    \n",
    "    # Forward fill then backward fill\n",
    "    df_combined = df_combined.fillna(method='ffill').fillna(method='bfill')\n",
    "else:\n",
    "    print(f\"\\nâœ… No remaining NaN values\")\n",
    "\n",
    "# Replace infinite values\n",
    "df_combined = df_combined.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill remaining NaNs with mean of numeric columns only\n",
    "numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if df_combined[col].isnull().any():\n",
    "        df_combined[col].fillna(df_combined[col].mean(), inplace=True)\n",
    "\n",
    "print(f\"\\nâœ… Data cleaned\")\n",
    "print(f\"   Final shape: {df_combined.shape}\")\n",
    "print(f\"   Remaining NaN: {df_combined.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f385ed95",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3692715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features separated from target\n",
      "   Feature columns: 108\n",
      "   Target variable: Modal_Price\n",
      "   Target shape: (14905,)\n",
      "\n",
      "âœ… Features normalized (StandardScaler)\n",
      "   Numeric features normalized: 98\n",
      "   Sample statistics after scaling:\n",
      "      Mean: 0.000000\n",
      "      Std: 1.000034\n",
      "\n",
      "âœ… Scaler saved to: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "drop_cols = ['Commodity', 'Market', 'Variety', 'Arrival_Date', 'date', 'Commodity_Market',\n",
    "             'Min_Price', 'Max_Price', 'Arrival']\n",
    "\n",
    "feature_cols = [col for col in df_combined.columns if col not in drop_cols + ['Modal_Price']]\n",
    "X = df_combined[feature_cols].copy()\n",
    "y = df_combined['Modal_Price'].copy()\n",
    "\n",
    "print(f\"âœ… Features separated from target\")\n",
    "print(f\"   Feature columns: {len(feature_cols)}\")\n",
    "print(f\"   Target variable: Modal_Price\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "\n",
    "# Normalize numeric features\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "scaler = StandardScaler()\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "print(f\"\\nâœ… Features normalized (StandardScaler)\")\n",
    "print(f\"   Numeric features normalized: {len(numeric_features)}\")\n",
    "print(f\"   Sample statistics after scaling:\")\n",
    "print(f\"      Mean: {X[numeric_features].mean().mean():.6f}\")\n",
    "print(f\"      Std: {X[numeric_features].std().mean():.6f}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_file = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\scaler.pkl\"\n",
    "with open(scaler_file, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"\\nâœ… Scaler saved to: {scaler_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94775988",
   "metadata": {},
   "source": [
    "## STEP 11: Save Engineered Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9cfca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Engineered dataset saved\n",
      "   File: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Processed_Data\\Features_Engineered.csv\n",
      "   Shape: (14905, 112)\n",
      "   Records: 14,905\n",
      "   Features: 108\n"
     ]
    }
   ],
   "source": [
    "# Create final dataset\n",
    "df_final = pd.concat([X, y.reset_index(drop=True)], axis=1)\n",
    "df_final['Arrival_Date'] = df_combined['Arrival_Date'].values\n",
    "df_final['Commodity'] = df_combined['Commodity'].values\n",
    "df_final['Market'] = df_combined['Market'].values\n",
    "\n",
    "# Save engineered dataset\n",
    "output_file = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Processed_Data\\Features_Engineered.csv\"\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"âœ… Engineered dataset saved\")\n",
    "print(f\"   File: {output_file}\")\n",
    "print(f\"   Shape: {df_final.shape}\")\n",
    "print(f\"   Records: {len(df_final):,}\")\n",
    "print(f\"   Features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a6a94",
   "metadata": {},
   "source": [
    "## STEP 12: Feature Documentation & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26edcb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Total Features: 108\n",
      "\n",
      "Lagged Price Features: 12 features\n",
      "  Examples: ['Price_Lag_1', 'Min_Price_Lag_1', 'Max_Price_Lag_1']\n",
      "Rolling Statistics: 16 features\n",
      "  Examples: ['Price_MA_7', 'Price_Std_7', 'Price_MA_14']\n",
      "Momentum Features: 3 features\n",
      "  Examples: ['Price_Change_1d_%', 'Price_Change_7d_%', 'Price_Change_30d_%']\n",
      "Seasonal Features: 8 features\n",
      "  Examples: ['Day_of_Week', 'Week_of_Year', 'Is_Weekend']\n",
      "Weather Features: 31 features\n",
      "  Examples: ['Temp_Min', 'Temp_Max', 'Temp_Mean']\n",
      "Business Features: 6 features\n",
      "  Examples: ['Commodity_Avg_Price', 'Market_Avg_Price', 'Comm_Market_Avg_Price']\n",
      "Categorical Features: 12 features\n",
      "  Examples: ['Day_of_Week', 'Day_of_Year', 'Grade_Medium']\n",
      "\n",
      "================================================================================\n",
      "Dataset Statistics:\n",
      "  Records: 14,905\n",
      "  Features: 108\n",
      "  Missing Values: 0\n",
      "  Target Variable: Modal_Price (Rs/Quintal)\n",
      "  Ready for ML: YES âœ“\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create feature documentation\n",
    "feature_types = {\n",
    "    'Lagged Price Features': [col for col in feature_cols if 'Lag' in col and 'Price' in col],\n",
    "    'Rolling Statistics': [col for col in feature_cols if 'MA_' in col or 'Std_' in col],\n",
    "    'Momentum Features': [col for col in feature_cols if 'Change' in col],\n",
    "    'Seasonal Features': [col for col in feature_cols if any(x in col for x in ['Season', 'Sin', 'Cos', 'Week'])],\n",
    "    'Weather Features': [col for col in feature_cols if any(x in col for x in ['Temp', 'Wind', 'Cloud', 'Rainfall'])],\n",
    "    'Business Features': [col for col in feature_cols if any(x in col for x in ['Commodity_Avg', 'Market_Avg', 'Comm_Market'])],\n",
    "    'Categorical Features': [col for col in feature_cols if 'Grade_' in col or 'Day_' in col or 'Encoded' in col]\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(f\"=\"*80)\n",
    "print(f\"\\nTotal Features: {len(feature_cols)}\\n\")\n",
    "\n",
    "for category, cols in feature_types.items():\n",
    "    print(f\"{category}: {len(cols)} features\")\n",
    "    if len(cols) > 0:\n",
    "        print(f\"  Examples: {cols[:3]}\")\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"  Records: {len(df_final):,}\")\n",
    "print(f\"  Features: {len(feature_cols)}\")\n",
    "print(f\"  Missing Values: {df_final.isnull().sum().sum()}\")\n",
    "print(f\"  Target Variable: Modal_Price (Rs/Quintal)\")\n",
    "print(f\"  Ready for ML: YES âœ“\")\n",
    "print(f\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c93f0",
   "metadata": {},
   "source": [
    "## STEP 13: Feature Validation & Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81e0c3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 15 Features by Correlation with Modal_Price:\n",
      "Price_MA_7                   0.972150\n",
      "Price_Lag_1                  0.962959\n",
      "Price_Max_7                  0.958074\n",
      "Month_Commodity_Avg_Price    0.955239\n",
      "Price_MA_14                  0.951207\n",
      "Price_Min_7                  0.948516\n",
      "Price_Max_14                 0.930255\n",
      "Max_Price_Lag_1              0.930237\n",
      "Price_MA_30                  0.923496\n",
      "Price_Min_14                 0.910769\n",
      "Min_Price_Lag_1              0.890732\n",
      "Price_Max_30                 0.889585\n",
      "Price_Lag_7                  0.874406\n",
      "Price_Min_30                 0.862125\n",
      "Comm_Market_Avg_Price        0.861518\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Bottom 15 Features by Correlation with Modal_Price:\n",
      "Is_Weekend                0.008910\n",
      "DayOfWeek_Cos             0.002837\n",
      "Year                     -0.004766\n",
      "DayOfWeek_Sin            -0.007478\n",
      "Day                      -0.011166\n",
      "Market_Commodity_Count   -0.027595\n",
      "Temp_Range_30            -0.044004\n",
      "Month_Sin                -0.060101\n",
      "Temp_Range_14            -0.066309\n",
      "Temp_Range_7             -0.076883\n",
      "Month_Cos                -0.084876\n",
      "Commodity_Volatility     -0.141975\n",
      "Price_Volatility_%       -0.142032\n",
      "Price_Range_Normalized   -0.142032\n",
      "Comm_Market_Records      -0.160001\n",
      "dtype: float64\n",
      "\n",
      "âœ… Feature correlations saved to: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Processed_Data\\Feature_Correlations.csv\n"
     ]
    }
   ],
   "source": [
    "# Compute correlations with target (numeric features only)\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "correlations = X_numeric.corrwith(y).sort_values(ascending=False)\n",
    "\n",
    "print(f\"\\nTop 15 Features by Correlation with Modal_Price:\")\n",
    "print(correlations.head(15))\n",
    "\n",
    "print(f\"\\n\\nBottom 15 Features by Correlation with Modal_Price:\")\n",
    "print(correlations.tail(15))\n",
    "\n",
    "# Save correlations\n",
    "corr_file = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Processed_Data\\Feature_Correlations.csv\"\n",
    "correlations.to_csv(corr_file, header=False)\n",
    "print(f\"\\nâœ… Feature correlations saved to: {corr_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb77214",
   "metadata": {},
   "source": [
    "## FINAL STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dee09cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘           PHASE 2: FEATURE ENGINEERING COMPLETE              â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "âœ… DELIVERABLES:\n",
      "   â€¢ Features_Engineered.csv - 14,905 records Ã— 108 features\n",
      "   â€¢ Feature_Correlations.csv - Correlation analysis\n",
      "   â€¢ scaler.pkl - StandardScaler for inference\n",
      "\n",
      "ğŸ“Š FEATURE BREAKDOWN:\n",
      "   â€¢ Lagged Features: 12\n",
      "   â€¢ Rolling Statistics: 16\n",
      "   â€¢ Momentum Features: 3\n",
      "   â€¢ Seasonal Features: 8\n",
      "   â€¢ Weather Features: 31\n",
      "   â€¢ Business Features: 6\n",
      "   â€¢ Categorical Features: 12\n",
      "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "   â€¢ TOTAL: 108 features\n",
      "\n",
      "ğŸ¯ DATA QUALITY:\n",
      "   â€¢ Records: 14,905\n",
      "   â€¢ Missing Values: 0\n",
      "   â€¢ Normalized: YES âœ“\n",
      "   â€¢ Ready for ML: YES âœ“\n",
      "\n",
      "â­ï¸  NEXT: Phase 3 - ML Model Development\n",
      "   Start with: PHASE_3_ML_Models.ipynb\n",
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘    ğŸš€ PHASE 2 COMPLETE - READY FOR ML MODEL TRAINING ğŸš€      â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘           PHASE 2: FEATURE ENGINEERING COMPLETE              â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ… DELIVERABLES:\n",
    "   â€¢ Features_Engineered.csv - {len(df_final):,} records Ã— {len(feature_cols)} features\n",
    "   â€¢ Feature_Correlations.csv - Correlation analysis\n",
    "   â€¢ scaler.pkl - StandardScaler for inference\n",
    "\n",
    "ğŸ“Š FEATURE BREAKDOWN:\n",
    "   â€¢ Lagged Features: {len(feature_types['Lagged Price Features'])}\n",
    "   â€¢ Rolling Statistics: {len(feature_types['Rolling Statistics'])}\n",
    "   â€¢ Momentum Features: {len(feature_types['Momentum Features'])}\n",
    "   â€¢ Seasonal Features: {len(feature_types['Seasonal Features'])}\n",
    "   â€¢ Weather Features: {len(feature_types['Weather Features'])}\n",
    "   â€¢ Business Features: {len(feature_types['Business Features'])}\n",
    "   â€¢ Categorical Features: {len(feature_types['Categorical Features'])}\n",
    "   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "   â€¢ TOTAL: {len(feature_cols)} features\n",
    "\n",
    "ğŸ¯ DATA QUALITY:\n",
    "   â€¢ Records: {len(df_final):,}\n",
    "   â€¢ Missing Values: {df_final.isnull().sum().sum()}\n",
    "   â€¢ Normalized: YES âœ“\n",
    "   â€¢ Ready for ML: YES âœ“\n",
    "\n",
    "â­ï¸  NEXT: Phase 3 - ML Model Development\n",
    "   Start with: PHASE_3_ML_Models.ipynb\n",
    "\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘    ğŸš€ PHASE 2 COMPLETE - READY FOR ML MODEL TRAINING ğŸš€      â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgriCast (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
