{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bcb1a7f",
   "metadata": {},
   "source": [
    "# Phase 3: ML Model Development for Commodity Price Prediction\n",
    "This notebook develops and compares multiple machine learning models using engineered features from Phase 2.\n",
    "\n",
    "**Models Covered:** Linear Regression, Random Forest, XGBoost, Neural Network (Deep Learning)\n",
    "**Workflow:** Data loading, train-test split, model training, evaluation, hyperparameter tuning, feature importance, final model selection and saving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f36127",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Load Engineered Features\n",
    "Import necessary libraries and load the engineered dataset and scaler from Phase 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ae07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load engineered features and scaler\n",
    "features_file = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Processed_Data\\Features_Engineered.csv\"\n",
    "scaler_file = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\scaler.pkl\"\n",
    "\n",
    "df = pd.read_csv(features_file)\n",
    "with open(scaler_file, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "print(f\"✅ Data and scaler loaded. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651af9e7",
   "metadata": {},
   "source": [
    "## 2. Train-Test Split & Data Preparation\n",
    "Split the data into training and testing sets using a time-based split. Display statistics and class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "drop_cols = ['Commodity', 'Market', 'Variety', 'Arrival_Date', 'date', 'Commodity_Market', 'Min_Price', 'Max_Price', 'Arrival']\n",
    "feature_cols = [col for col in df.columns if col not in drop_cols + ['Modal_Price']]\n",
    "X = df[feature_cols].copy()\n",
    "y = df['Modal_Price'].copy()\n",
    "\n",
    "# Time-based split (assume sorted by Arrival_Date)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "print(f\"Target mean (train): {y_train.mean():.2f}, (test): {y_test.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f23c1dd",
   "metadata": {},
   "source": [
    "## 3. Baseline Model (Linear Regression)\n",
    "Train a linear regression model as baseline. Evaluate using MAE, RMSE, R² score and visualize predictions vs actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b1f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = mean_squared_error(y_test, y_pred_lr, squared=False)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Linear Regression MAE: {mae_lr:.2f}\")\n",
    "print(f\"Linear Regression RMSE: {rmse_lr:.2f}\")\n",
    "print(f\"Linear Regression R²: {r2_lr:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(y_test.values, label='Actual')\n",
    "plt.plot(y_pred_lr, label='Predicted')\n",
    "plt.title('Linear Regression: Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b648fd",
   "metadata": {},
   "source": [
    "## 4. Random Forest Regressor\n",
    "Train Random Forest regressor. Evaluate performance metrics and compare with baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Regressor\n",
    "rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest MAE: {mae_rf:.2f}\")\n",
    "print(f\"Random Forest RMSE: {rmse_rf:.2f}\")\n",
    "print(f\"Random Forest R²: {r2_rf:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(y_test.values, label='Actual')\n",
    "plt.plot(y_pred_rf, label='Predicted')\n",
    "plt.title('Random Forest: Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596369f",
   "metadata": {},
   "source": [
    "## 5. Gradient Boosting (XGBoost)\n",
    "Train XGBoost regressor with optimized parameters. Evaluate performance and compare with previous models. Display feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost MAE: {mae_xgb:.2f}\")\n",
    "print(f\"XGBoost RMSE: {rmse_xgb:.2f}\")\n",
    "print(f\"XGBoost R²: {r2_xgb:.3f}\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(y_test.values, label='Actual')\n",
    "plt.plot(y_pred_xgb, label='Predicted')\n",
    "plt.title('XGBoost: Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "xgb_importance = pd.Series(xgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "xgb_importance.head(20).plot(kind='bar')\n",
    "plt.title('Top 20 Feature Importances (XGBoost)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97d45c",
   "metadata": {},
   "source": [
    "## 6. Neural Network (Deep Learning)\n",
    "Build and train a neural network using TensorFlow/Keras. Monitor training/validation loss and evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a772a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=100, batch_size=64, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Neural Network Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred_nn = model.predict(X_test).flatten()\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "rmse_nn = mean_squared_error(y_test, y_pred_nn, squared=False)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\"Neural Network MAE: {mae_nn:.2f}\")\n",
    "print(f\"Neural Network RMSE: {rmse_nn:.2f}\")\n",
    "print(f\"Neural Network R²: {r2_nn:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989c8433",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation & Comparison\n",
    "Compare all models using MAE, RMSE, R² and MAPE metrics. Create comparison tables and visualizations. Identify best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49753946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Neural Network'],\n",
    "    'MAE': [mae_lr, mae_rf, mae_xgb, mae_nn],\n",
    "    'RMSE': [rmse_lr, rmse_rf, rmse_xgb, rmse_nn],\n",
    "    'R2': [r2_lr, r2_rf, r2_xgb, r2_nn],\n",
    "    'MAPE': [mape(y_test, y_pred_lr), mape(y_test, y_pred_rf), mape(y_test, y_pred_xgb), mape(y_test, y_pred_nn)]\n",
    "})\n",
    "print(results)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Model', y='RMSE', data=results)\n",
    "plt.title('Model RMSE Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ab74e",
   "metadata": {},
   "source": [
    "## 8. Hyperparameter Tuning\n",
    "Perform GridSearchCV or RandomizedSearchCV on top 2-3 models to optimize hyperparameters. Document best parameters and performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: RandomizedSearchCV for Random Forest\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "rf_search = RandomizedSearchCV(RandomForestRegressor(random_state=42), param_dist, n_iter=10, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best RF Params: {rf_search.best_params_}\")\n",
    "print(f\"Best RF CV Score: {-rf_search.best_score_:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73e76e6",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis\n",
    "Extract and visualize feature importance from tree-based models. Compare with Phase 1 correlation analysis. Identify critical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98776e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf_importance = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10,6))\n",
    "rf_importance.head(20).plot(kind='bar')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.show()\n",
    "\n",
    "# Compare with XGBoost importance (already plotted above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca832509",
   "metadata": {},
   "source": [
    "## 10. Final Model Selection & Save\n",
    "Select best model based on evaluation metrics. Save the final model. Create prediction function for inference. Generate final performance report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and save best model (example: XGBoost)\n",
    "final_model = xgb_model\n",
    "model_file = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\final_model_xgb.pkl\"\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "print(f\"✅ Final model saved to: {model_file}\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_price(features, model=final_model, scaler=scaler):\n",
    "    features_scaled = scaler.transform([features])\n",
    "    return model.predict(features_scaled)[0]\n",
    "\n",
    "# Final performance report\n",
    "print(\"\\nFinal Model Performance:\")\n",
    "print(f\"MAE: {mae_xgb:.2f}\")\n",
    "print(f\"RMSE: {rmse_xgb:.2f}\")\n",
    "print(f\"R²: {r2_xgb:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
