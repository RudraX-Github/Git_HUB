{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mandi Data api Key = 579b464db66ec23bdd000001e3a48235641a458b502501e95c36f78e\n",
    "\n",
    "Mandi Data file = \"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Mandi_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9945838",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weatherbit API\n",
    "\n",
    "Example API URL: https://api.weatherbit.io/v2.0/history/subhourly?lat=35.78&lon=78.64&start_date=2025-11-11&end_date=2025-11-12&key=API_KEY\n",
    "\n",
    "Master API Key = de8e274a65ec45929d501a63466da6cf\t\n",
    "\n",
    "(Plan\tDetails\tSupport Level\n",
    "Business Trial (Expires 2025-12-03)\n",
    "1,500 req/day\n",
    "1,500 historical req/day\n",
    "25 years historical\n",
    "Current weather + alerts + lightning\n",
    "Daily forecasts\n",
    "Hourly forecasts\n",
    "60 minute forecasts\n",
    "+ Energy / Air Quality / Agweather / Climate Normals API\n",
    "Non-Commercial use only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1328c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Weatherbit Batch Scraper ---\n",
      "Found 33 markets to process.\n",
      "No progress file found, starting from scratch.\n",
      "\n",
      "--- Processing Market: Bardoli ---\n",
      "Period: 2024-01-01 to 2024-12-31\n",
      "Total days to query for this market: 366.\n",
      "Creating new file: Bardoli_weather_2024-01-01_to_2024-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Bardoli:   0%|          | 0/366 [00:00<?, ?it/s, Key: ...a6cf]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Key ...f15c failed (Rate Limit/Auth). Trying next key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Bardoli: 100%|██████████| 366/366 [06:18<00:00,  1.03s/it, Key: ...a6cf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fetched 366 total records for Bardoli.\n",
      "Processing data into flat DataFrame (using json_normalize)...\n",
      "Dropped duplicates, 366 unique records remain.\n",
      "Dropping columns: ['revision_version']\n",
      "Saving data to Bardoli_weather_2024-01-01_to_2024-12-31.csv...\n",
      "\n",
      "Successfully saved all data for Bardoli to Bardoli_weather_2024-01-01_to_2024-12-31.csv\n",
      "--- Completed market: Bardoli. Marked as 'completed'. ---\n",
      "\n",
      "--- Processing Market: Bardoli_Katod ---\n",
      "Period: 2024-01-01 to 2024-12-31\n",
      "Total days to query for this market: 366.\n",
      "Creating new file: Bardoli_Katod_weather_2024-01-01_to_2024-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Bardoli_Katod: 100%|██████████| 366/366 [06:12<00:00,  1.02s/it, Key: ...a6cf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fetched 366 total records for Bardoli_Katod.\n",
      "Processing data into flat DataFrame (using json_normalize)...\n",
      "Dropped duplicates, 366 unique records remain.\n",
      "Dropping columns: ['revision_version']\n",
      "Saving data to Bardoli_Katod_weather_2024-01-01_to_2024-12-31.csv...\n",
      "\n",
      "Successfully saved all data for Bardoli_Katod to Bardoli_Katod_weather_2024-01-01_to_2024-12-31.csv\n",
      "--- Completed market: Bardoli_Katod. Marked as 'completed'. ---\n",
      "\n",
      "--- Processing Market: Bardoli_Madhi ---\n",
      "Period: 2024-01-01 to 2024-12-31\n",
      "Total days to query for this market: 366.\n",
      "Creating new file: Bardoli_Madhi_weather_2024-01-01_to_2024-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Bardoli_Madhi: 100%|██████████| 366/366 [06:11<00:00,  1.01s/it, Key: ...a6cf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fetched 366 total records for Bardoli_Madhi.\n",
      "Processing data into flat DataFrame (using json_normalize)...\n",
      "Dropped duplicates, 366 unique records remain.\n",
      "Dropping columns: ['revision_version']\n",
      "Saving data to Bardoli_Madhi_weather_2024-01-01_to_2024-12-31.csv...\n",
      "\n",
      "Successfully saved all data for Bardoli_Madhi to Bardoli_Madhi_weather_2024-01-01_to_2024-12-31.csv\n",
      "--- Completed market: Bardoli_Madhi. Marked as 'completed'. ---\n",
      "\n",
      "--- Processing Market: Kosamba ---\n",
      "Period: 2024-01-01 to 2024-12-31\n",
      "Total days to query for this market: 366.\n",
      "Creating new file: Kosamba_weather_2024-01-01_to_2024-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Kosamba:  60%|██████    | 220/366 [04:05<02:30,  1.03s/it, Key: ...a6cf]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Network error on 2024-08-08: HTTPSConnectionPool(host='api.weatherbit.io', port=443): Max retries exceeded with url: /v2.0/history/daily?lat=21.4644599234544&lon=72.9520890103333&start_date=2024-08-08&end_date=2024-08-09&key=de8e274a65ec45929d501a63466da6cf (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000185D4546310>, 'Connection to api.weatherbit.io timed out. (connect timeout=None)')). Retrying after 10s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Kosamba: 100%|██████████| 366/366 [06:44<00:00,  1.11s/it, Key: ...a6cf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully fetched 366 total records for Kosamba.\n",
      "Processing data into flat DataFrame (using json_normalize)...\n",
      "Dropped duplicates, 366 unique records remain.\n",
      "Dropping columns: ['revision_version']\n",
      "Saving data to Kosamba_weather_2024-01-01_to_2024-12-31.csv...\n",
      "\n",
      "Successfully saved all data for Kosamba to Kosamba_weather_2024-01-01_to_2024-12-31.csv\n",
      "--- Completed market: Kosamba. Marked as 'completed'. ---\n",
      "\n",
      "--- Processing Market: Kosamba_Vankal ---\n",
      "Period: 2024-01-01 to 2024-12-31\n",
      "Total days to query for this market: 366.\n",
      "Creating new file: Kosamba_Vankal_weather_2024-01-01_to_2024-12-31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Kosamba_Vankal:  10%|▉         | 36/366 [00:37<05:39,  1.03s/it, Key: ...92e0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Key ...a6cf failed (Rate Limit/Auth). Trying next key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Kosamba_Vankal:  37%|███▋      | 135/366 [02:15<03:51,  1.00s/it, Key: ...92e0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Key ...92e0 failed (Rate Limit/Auth). Trying next key.\n",
      "\n",
      "CRITICAL: All API keys are exhausted. Stopping script.\n",
      "Run the script again tomorrow (or add new keys) to resume.\n",
      "Batch process stopped due to API key exhaustion.\n",
      "Run the script again later to resume from the last checkpoint.\n",
      "--- Batch processing finished. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1. ADD YOUR API KEYS HERE\n",
    "# It will rotate through this list if one key hits its rate limit.\n",
    "API_KEYS = [\n",
    "    \"de8e274a65ec45929d501a63466da6cf\", # Business Trial 1 (Expires 2025-12-03)\n",
    "    \"7335451fcc0646bb86611027b68292e0\", # Business Trial 2 (Expires 2025-12-07)\n",
    "    \"9e56a4f3fab748418bc299713138f15c\", # Business Trial 3 (Expires 2025-12-07)\n",
    "    \"fa82e8af08314f64ab0d2f7492b46e9f\", # Business Trial 4 (Expires 2025-12-07)\n",
    "    \"680cfd56aefc45d6bfa1d40fb65a3028\"  # Business Trial 5 (Expires 2025-12-08)\n",
    "]\n",
    "# Global index to track which key we are currently using\n",
    "current_key_index = 0\n",
    "\n",
    "# 2. ADD YOUR MARKETS HERE\n",
    "# The script will loop through this dictionary and create one CSV per market.\n",
    "MARKETS = {\n",
    "    \"Bardoli\": {\"lat\": 21.1439076246341, \"lon\": 73.249972681491997},\n",
    "    \"Bardoli_Katod\": {\"lat\": 21.12, \"lon\": 73.12},\n",
    "    \"Bardoli_Madhi\": {\"lat\": 21.15, \"lon\": 73.25},\n",
    "    \"Kosamba\": {\"lat\": 21.464459923454399, \"lon\": 72.952089010333296},\n",
    "    \"Kosamba_Vankal\": {\"lat\": 21.43, \"lon\": 73.23},\n",
    "    \"Kosamba_Zangvav\": {\"lat\": 21.48, \"lon\": 72.95},\n",
    "    \"Mahuva\": {\"lat\": 21.097129557468101, \"lon\": 71.760557527893894},\n",
    "    \"Mahuva_Anaval\": {\"lat\": 20.84, \"lon\": 73.26},\n",
    "    \"Mandvi\": {\"lat\": 21.259469954916099, \"lon\": 73.306064794408599},\n",
    "    \"Nizar\": {\"lat\": 21.47, \"lon\": 74.19},\n",
    "    \"Nizar_Kukarmuda\": {\"lat\": 21.51, \"lon\": 74.13},\n",
    "    \"Nizar_Pumkitalov\": {\"lat\": 21.47, \"lon\": 74.10},\n",
    "    \"Songadh\": {\"lat\": 21.175868577082898, \"lon\": 73.564033008192197},\n",
    "    \"Songadh_Badarpada\": {\"lat\": 21.16, \"lon\": 73.56},\n",
    "    \"Songadh_Umrada\": {\"lat\": 21.16, \"lon\": 73.56},\n",
    "    \"Surat\": {\"lat\": 21.193417012914299, \"lon\": 72.852982768754401},\n",
    "    \"Uchhal\": {\"lat\": 21.17, \"lon\": 73.74},\n",
    "    \"Valod_Buhari\": {\"lat\": 20.97, \"lon\": 73.31},\n",
    "    \"Vyara_Paati\": {\"lat\": 21.11, \"lon\": 73.38},\n",
    "    \"Vyra\": {\"lat\": 21.112412216859902, \"lon\": 73.388557572057493},\n",
    "    \"Amreli\": {\"lat\": 21.559338614206901, \"lon\": 71.227430257825006},\n",
    "    \"Babra\": {\"lat\": 21.848355060711398, \"lon\": 71.311297950245503},\n",
    "    \"Bagasara\": {\"lat\": 21.496945117777699, \"lon\": 70.959329215009802},\n",
    "    \"Dhari\": {\"lat\": 21.331365216974699, \"lon\": 71.023828198856805},\n",
    "    \"Rajula\": {\"lat\": 21.0339169150099, \"lon\": 71.443913765569206},\n",
    "    \"Savarkundla\": {\"lat\": 21.332621199094, \"lon\": 71.313830460040407},\n",
    "    \"Ahmedabad_Chimanbhai_Patal_Market_Vasana\": {\"lat\": 22.996976855033001, \"lon\": 72.536392460074595},\n",
    "    \"Bavla\": {\"lat\": 22.830965521422801, \"lon\": 72.3579378735662},\n",
    "    \"Dhandhuka\": {\"lat\": 22.377117706820101, \"lon\": 71.978523283421794},\n",
    "    \"Dholka\": {\"lat\": 22.7212505077608, \"lon\": 72.448905310358995},\n",
    "    \"Mandal\": {\"lat\": 23.282162015159599, \"lon\": 71.915345532018506},\n",
    "    \"Sanad\": {\"lat\": 22.997609695821499, \"lon\": 72.381634273951903},\n",
    "    \"Viramgam\": {\"lat\": 23.125945592319901, \"lon\": 72.045712191155303}\n",
    "}\n",
    "\n",
    "# 3. SETTINGS\n",
    "BASE_URL = \"https://api.weatherbit.io/v2.0/history/daily\"\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-12-31'\n",
    "\n",
    "# Columns to remove from the final CSV\n",
    "COLUMNS_TO_DROP = ['snow_rate', 'weather.icon', 'revision_version', 'timestamp_utc']\n",
    "\n",
    "# Checkpoint file to resume progress\n",
    "PROGRESS_FILE = 'scrape_progress.json'\n",
    "\n",
    "# --- END CONFIGURATION ---\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Loads the progress file if it exists.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        print(f\"Loading progress from {PROGRESS_FILE}...\")\n",
    "        with open(PROGRESS_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    print(\"No progress file found, starting from scratch.\")\n",
    "    return {}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Saves the current progress to the checkpoint file.\"\"\"\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=4)\n",
    "\n",
    "def fetch_weather_for_market(market_name, lat, lon, start_date_str, end_date_str, output_file, progress):\n",
    "    \"\"\"\n",
    "    Fetches weather data for a single market, starting from start_date_str.\n",
    "    Includes API key rotation and saves progress *per day*.\n",
    "    \"\"\"\n",
    "    global current_key_index\n",
    "    \n",
    "    print(f\"\\n--- Processing Market: {market_name} ---\")\n",
    "    print(f\"Period: {start_date_str} to {end_date_str}\")\n",
    "    \n",
    "    try:\n",
    "        # Create the date range to iterate over\n",
    "        date_range = pd.date_range(start=start_date_str, end=end_date_str, freq='D')\n",
    "    except Exception as e:\n",
    "        print(f\"Error: Invalid date range. Start: {start_date_str}, End: {END_DATE}. {e}\")\n",
    "        return False # Signal failure\n",
    "\n",
    "    if date_range.empty:\n",
    "        print(\"Date range is empty, nothing to fetch.\")\n",
    "        return True # Signal success (nothing to do)\n",
    "\n",
    "    all_data = [] # Master list for this market's records\n",
    "    \n",
    "    print(f\"Total days to query for this market: {len(date_range)}.\")\n",
    "    \n",
    "    # Check if a partial CSV already exists from a previous run\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Found existing file: {output_file}. Appending new data.\")\n",
    "        try:\n",
    "            # Load existing data to avoid re-writing it\n",
    "            df_existing = pd.read_csv(output_file)\n",
    "            # Convert to list of dicts to easily append\n",
    "            all_data = df_existing.to_dict('records')\n",
    "            print(f\"Loaded {len(all_data)} existing records.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read existing CSV {output_file}. Will start fresh. Error: {e}\")\n",
    "            all_data = []\n",
    "    else:\n",
    "        print(f\"Creating new file: {output_file}\")\n",
    "    \n",
    "    # Use tqdm for a nice progress bar\n",
    "    pbar = tqdm(date_range, desc=f\"Fetching for {market_name}\")\n",
    "    for day_start in pbar:\n",
    "        \n",
    "        current_start_date = day_start.strftime('%Y-%m-%d')\n",
    "        current_end_date = (day_start + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        day_fetched = False\n",
    "        \n",
    "        # Loop to handle API key rotation and retries for a single day\n",
    "        while not day_fetched:\n",
    "            if current_key_index >= len(API_KEYS):\n",
    "                tqdm.write(\"\\nCRITICAL: All API keys are exhausted. Stopping script.\")\n",
    "                tqdm.write(\"Run the script again tomorrow (or add new keys) to resume.\")\n",
    "                return False # Signal failure (keys exhausted)\n",
    "\n",
    "            # Get the current API key\n",
    "            current_key = API_KEYS[current_key_index]\n",
    "            pbar.set_postfix_str(f\"Key: ...{current_key[-4:]}\")\n",
    "\n",
    "            params = {\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'start_date': current_start_date,\n",
    "                'end_date': current_end_date,\n",
    "                'key': current_key\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                # Make the API request\n",
    "                response = requests.get(BASE_URL, params=params)\n",
    "                \n",
    "                # Check for HTTP errors (4xx, 5xx)\n",
    "                response.raise_for_status() \n",
    "                \n",
    "                # Extract the JSON data\n",
    "                data = response.json()\n",
    "                \n",
    "                # The actual weather records are in the 'data' key\n",
    "                if 'data' in data and data['data']:\n",
    "                    all_data.extend(data['data'])\n",
    "                else:\n",
    "                    tqdm.write(f\"Warning: No data found for {current_start_date}\")\n",
    "\n",
    "                day_fetched = True # Success! Move to the next day\n",
    "                \n",
    "                # --- THIS IS THE CHECKPOINT ---\n",
    "                # Save progress *after* a successful day fetch\n",
    "                next_start_date = (day_start + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "                progress[market_name] = next_start_date\n",
    "                save_progress(progress)\n",
    "                # -----------------------------\n",
    "\n",
    "                time.sleep(0.2) # Be polite to the API\n",
    "\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code in [403, 429]:\n",
    "                    # 403 (Forbidden) or 429 (Too Many Requests)\n",
    "                    tqdm.write(f\"\\nWarning: Key ...{current_key[-4:]} failed (Rate Limit/Auth). Trying next key.\")\n",
    "                    current_key_index += 1 # Move to the next key\n",
    "                    # The loop will retry this day with the new key\n",
    "                else:\n",
    "                    tqdm.write(f\"\\nHTTP Error on {current_start_date}: {e}. Retrying after 10s...\")\n",
    "                    time.sleep(10)\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                tqdm.write(f\"\\nNetwork error on {current_start_date}: {e}. Retrying after 10s...\")\n",
    "                time.sleep(10)\n",
    "            \n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"\\nAn unexpected error occurred on {current_start_date}: {e}. Skipping day.\")\n",
    "                break # Stop trying this day and move on\n",
    "\n",
    "    # --- End of date loop ---\n",
    "\n",
    "    if not all_data:\n",
    "        print(f\"No data was fetched for {market_name}. Check parameters and API plan.\")\n",
    "        return True # Return True to continue to the next market\n",
    "\n",
    "    print(f\"\\nSuccessfully fetched {len(all_data)} total records for {market_name}.\")\n",
    "    \n",
    "    # Process the data with Pandas\n",
    "    try:\n",
    "        print(\"Processing data into flat DataFrame (using json_normalize)...\")\n",
    "        # Use json_normalize for robust nested JSON handling\n",
    "        df = pd.json_normalize(all_data)\n",
    "        \n",
    "        # Remove duplicate rows that might have come from re-runs\n",
    "        # Use 'timestamp' or 'ts' if available, 'datetime' is good for daily\n",
    "        if 'datetime' in df.columns:\n",
    "            df = df.drop_duplicates(subset=['datetime'])\n",
    "            print(f\"Dropped duplicates, {len(df)} unique records remain.\")\n",
    "        \n",
    "        # --- Drop unwanted columns ---\n",
    "        # Check which of the columns to drop actually exist in the DataFrame\n",
    "        existing_cols_to_drop = [col for col in COLUMNS_TO_DROP if col in df.columns]\n",
    "        if existing_cols_to_drop:\n",
    "            print(f\"Dropping columns: {existing_cols_to_drop}\")\n",
    "            df = df.drop(columns=existing_cols_to_drop)\n",
    "        else:\n",
    "            print(\"No columns to drop were found.\")\n",
    "        # --- End of drop ---\n",
    "        \n",
    "        print(f\"Saving data to {output_file}...\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\nSuccessfully saved all data for {market_name} to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data with Pandas or saving to CSV: {e}\")\n",
    "        \n",
    "    return True # Signal success, move to next market\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to iterate over markets and call the fetcher.\n",
    "    Manages overall progress and resumes from checkpoints.\n",
    "    \"\"\"\n",
    "    # This try block handles the --f=... argument from jupyter\n",
    "    try:\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser()\n",
    "        # We don't add any arguments, we just want to parse_known_args\n",
    "        # to ignore jupyter's --f argument\n",
    "        parser.parse_known_args()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if not MARKETS:\n",
    "        print(\"Error: The 'MARKETS' dictionary is empty. Please add markets to scrape.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    if not API_KEYS:\n",
    "        print(\"Error: The 'API_KEYS' list is empty. Please add at least one API key.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    print(f\"--- Starting Weatherbit Batch Scraper ---\")\n",
    "    print(f\"Found {len(MARKETS)} markets to process.\")\n",
    "    \n",
    "    # Load progress from checkpoint file\n",
    "    progress = load_progress()\n",
    "    \n",
    "    # Loop over each market defined in the config\n",
    "    for market_name, coords in MARKETS.items():\n",
    "        \n",
    "        # Check the status of this market\n",
    "        market_status = progress.get(market_name)\n",
    "        \n",
    "        if market_status == \"completed\":\n",
    "            print(f\"Skipping '{market_name}': Already marked as 'completed'.\")\n",
    "            continue\n",
    "            \n",
    "        # Determine the start date for this market\n",
    "        # If it's not in progress, use the global START_DATE\n",
    "        # If it is in progress, use the date from the progress file\n",
    "        current_start_date = START_DATE if not market_status else market_status\n",
    "        \n",
    "        # Check if the start date is already past the end date (edge case)\n",
    "        if pd.to_datetime(current_start_date) > pd.to_datetime(END_DATE):\n",
    "            print(f\"Skipping '{market_name}': Progress file shows it is finished.\")\n",
    "            progress[market_name] = \"completed\"\n",
    "            save_progress(progress)\n",
    "            continue\n",
    "            \n",
    "        output_file = f\"{market_name}_weather_{START_DATE}_to_{END_DATE}.csv\"\n",
    "        \n",
    "        # Call the main fetching function for this market\n",
    "        success = fetch_weather_for_market(\n",
    "            market_name,\n",
    "            coords['lat'],\n",
    "            coords['lon'],\n",
    "            current_start_date, # Start from where we left off\n",
    "            END_DATE,\n",
    "            output_file,\n",
    "            progress # Pass the progress object to be updated\n",
    "        )\n",
    "        \n",
    "        if not success:\n",
    "            # This happens if all API keys are exhausted\n",
    "            print(\"Batch process stopped due to API key exhaustion.\")\n",
    "            print(\"Run the script again later to resume from the last checkpoint.\")\n",
    "            break # Stop iterating over markets\n",
    "        \n",
    "        # If the fetcher finished without exhausting keys, it means this market is done\n",
    "        progress[market_name] = \"completed\"\n",
    "        save_progress(progress)\n",
    "        print(f\"--- Completed market: {market_name}. Marked as 'completed'. ---\")\n",
    "        time.sleep(1) # Small delay between markets\n",
    "        \n",
    "    print(\"--- Batch processing finished. ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
