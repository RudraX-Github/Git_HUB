{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33448fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "/resource/35985678-0d79-46b4-9ed6-6f13308a1d24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mandi Data api\n",
    "\n",
    "https://api.data.gov.in/resource/35985678-0d79-46b4-9ed6-6f13308a1d24?api-key=579b464db66ec23bdd000001e3a48235641a458b502501e95c36f78e&format=json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf187cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNews API \n",
    "https://gnews.io/api/v4/{endpoint}?{parameters}&apikey=698746a0423c3a255985f83db1100e7c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9945838",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weather =bit API\n",
    "\n",
    "https://api.weatherbit.io/v2.0/history/subhourly?lat=35.78&lon=78.64&start_date=2025-11-07&end_date=2025-11-08&key=ea173c6e1835424e9668ea1d8a7a4b9b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59679757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 21 markets from Excel file.\n",
      "\n",
      "Starting API call for 21 markets (using Open-Meteo)...\n",
      "Querying URL: https://archive-api.open-Meteo.com/v1/archive\n",
      "Querying Dates: 2024-01-01 to 2025-01-01\n",
      "  > Successfully fetched data for 21 locations in one call.\n",
      "  > Processed data for: Bardoli\n",
      "  > Processed data for: Bardoli(Kat)\n",
      "  > Processed data for: Bardoli(Ma)\n",
      "  > Processed data for: Kosamba\n",
      "  > Processed data for: Kosamba(D)\n",
      "  > Processed data for: Mahuva\n",
      "  > Processed data for: Mahuva(Am)\n",
      "  > Processed data for: Mandvi\n",
      "  > Processed data for: Nizar\n",
      "  > Processed data for: Nizar(Kuka)\n",
      "  > Processed data for: Nizar(Pumb)\n",
      "  > Processed data for: S.Mandvi\n",
      "  > Processed data for: Songadh\n",
      "  > Processed data for: Songadh(B)\n",
      "  > Processed data for: Songadh(U)\n",
      "  > Processed data for: Surat\n",
      "  > Processed data for: Uchhal\n",
      "  > Processed data for: Valod\n",
      "  > Processed data for: Valod(Buh)\n",
      "  > Processed data for: Vyara(Pan)\n",
      "  > Processed data for: Vyra\n",
      "\n",
      "Writing all market data to Excel file: market_historical_weather_daily.xlsx\n",
      "  > Writing sheet: Bardoli...\n",
      "  > Writing sheet: Bardoli(Kat)...\n",
      "  > Writing sheet: Bardoli(Ma)...\n",
      "  > Writing sheet: Kosamba...\n",
      "  > Writing sheet: Kosamba(D)...\n",
      "  > Writing sheet: Mahuva...\n",
      "  > Writing sheet: Mahuva(Am)...\n",
      "  > Writing sheet: Mandvi...\n",
      "  > Writing sheet: Nizar...\n",
      "  > Writing sheet: Nizar(Kuka)...\n",
      "  > Writing sheet: Nizar(Pumb)...\n",
      "  > Writing sheet: S.Mandvi...\n",
      "  > Writing sheet: Songadh...\n",
      "  > Writing sheet: Songadh(B)...\n",
      "  > Writing sheet: Songadh(U)...\n",
      "  > Writing sheet: Surat...\n",
      "  > Writing sheet: Uchhal...\n",
      "  > Writing sheet: Valod...\n",
      "  > Writing sheet: Valod(Buh)...\n",
      "  > Writing sheet: Vyara(Pan)...\n",
      "  > Writing sheet: Vyra...\n",
      "\n",
      "Successfully collected all data and saved to 'market_historical_weather_daily.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- 1. Load and Parse the Market Excel File ---\n",
    "\n",
    "def parse_market_file(file_name):\n",
    "    \"\"\"\n",
    "    Reads the Excel file and extracts market coordinates\n",
    "    from the first column using regular expressions.\n",
    "    \"\"\"\n",
    "    market_data = []\n",
    "    # Regex to find \"MarketName\": (Lat, Lon)\n",
    "    pattern = re.compile(r'\"([^\"]+)\":\\s*\\(\\s*([0-9.-]+)\\s*,\\s*([0-9.-]+)\\s*\\)')\n",
    "    \n",
    "    try:\n",
    "        # Use read_excel for .xlsx files\n",
    "        df = pd.read_excel(file_name, engine='openpyxl')\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"The market file is empty.\")\n",
    "            return None\n",
    "            \n",
    "        column_name = df.columns[0]\n",
    "        \n",
    "        for line in df[column_name]:\n",
    "            if not isinstance(line, str):\n",
    "                continue\n",
    "                \n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                market_data.append({\n",
    "                    \"Market_Name\": match.group(1),\n",
    "                    \"Latitude\": float(match.group(2)),\n",
    "                    \"Longitude\": float(match.group(3))\n",
    "                })\n",
    "                \n",
    "        if not market_data:\n",
    "            print(\"Could not parse any market data from the file.\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Successfully parsed {len(market_data)} markets from Excel file.\")\n",
    "        return pd.DataFrame(market_data)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{file_name}' was not found.\")\n",
    "        return None\n",
    "    except ImportError:\n",
    "        print(\"Error: The 'openpyxl' library is required to read .xlsx files.\")\n",
    "        print(\"Please install it: pip install openpyxl\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred parsing the file: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 2. Fetch Weather Data from Open-Meteo API ---\n",
    "\n",
    "def fetch_weather_for_markets(df_markets):\n",
    "    \"\"\"\n",
    "    Fetches DAILY historical weather data from the Open-Meteo API\n",
    "    for the specified date range.\n",
    "    \n",
    "    This API is free, requires no key, and can handle all locations\n",
    "    and the full date range in a single API call.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open-Meteo Historical (Archive) API endpoint\n",
    "    BASE_URL = \"https://archive-api.open-Meteo.com/v1/archive\"\n",
    "    \n",
    "    # Your requested date range\n",
    "    START_DATE = \"2024-01-01\"\n",
    "    END_DATE = \"2025-01-01\"\n",
    "    \n",
    "    # Define the daily weather variables we want.\n",
    "    # See docs for more: https://open-meteo.com/en/docs/historical-weather-api\n",
    "    DAILY_VARIABLES = [\n",
    "        \"temperature_2m_max\",\n",
    "        \"temperature_2m_min\",\n",
    "        \"temperature_2m_mean\",\n",
    "        \"precipitation_sum\",\n",
    "        \"rain_sum\",\n",
    "        \"snowfall_sum\",\n",
    "        \"wind_speed_10m_max\",\n",
    "        \"wind_gusts_10m_max\",\n",
    "        \"weather_code\",         # <-- ADDED\n",
    "        \"cloud_cover_mean\",     # <-- ADDED\n",
    "        \"uv_index_max\"          # <-- ADDED\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nStarting API call for {len(df_markets)} markets (using Open-Meteo)...\")\n",
    "    print(f\"Querying URL: {BASE_URL}\")\n",
    "    print(f\"Querying Dates: {START_DATE} to {END_DATE}\")\n",
    "\n",
    "    # --- Build one API call for ALL markets ---\n",
    "    # Open-Meteo is powerful and can take arrays of lat/lon\n",
    "    \n",
    "    all_lats = df_markets['Latitude'].tolist()\n",
    "    all_lons = df_markets['Longitude'].tolist()\n",
    "    all_market_names = df_markets['Market_Name'].tolist()\n",
    "\n",
    "    params = {\n",
    "        'latitude': all_lats,\n",
    "        'longitude': all_lons,\n",
    "        'start_date': START_DATE,\n",
    "        'end_date': END_DATE,\n",
    "        'daily': DAILY_VARIABLES,\n",
    "        'timezone': 'auto' # Automatically adjust to the local timezone\n",
    "    }\n",
    "\n",
    "    all_market_dataframes = [] # List to hold final dataframes\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        response.raise_for_status() # Raise error for bad responses\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            # If only one location, API returns a single object. Wrap it in a list.\n",
    "            data = [data]\n",
    "\n",
    "        print(f\"  > Successfully fetched data for {len(data)} locations in one call.\")\n",
    "\n",
    "        # --- Process the response ---\n",
    "        # `data` is a list, each item corresponds to one location\n",
    "        for i, location_data in enumerate(data):\n",
    "            market_name = all_market_names[i]\n",
    "            lat = all_lats[i]\n",
    "            lon = all_lons[i]\n",
    "            \n",
    "            daily_data = location_data.get('daily')\n",
    "            if not daily_data:\n",
    "                print(f\"  > No 'daily' data returned for: {market_name}\")\n",
    "                continue\n",
    "\n",
    "            df_daily = pd.DataFrame(daily_data)\n",
    "            \n",
    "            # Add the market context to this dataframe\n",
    "            df_daily['market_name_lookup'] = market_name\n",
    "            df_daily['latitude_lookup'] = lat\n",
    "            df_daily['longitude_lookup'] = lon\n",
    "            \n",
    "            all_market_dataframes.append(df_daily)\n",
    "            print(f\"  > Processed data for: {market_name}\")\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"  > FAILED (HTTP Error): {http_err}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  > FAILED (Connection Error): {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  > FAILED (Processing Error): {e}\")\n",
    "\n",
    "    if not all_market_dataframes:\n",
    "        print(\"\\nNo weather data was collected from the API.\")\n",
    "        return None\n",
    "        \n",
    "    # Combine all individual market dataframes into one large dataframe\n",
    "    final_df = pd.concat(all_market_dataframes, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "# --- 3. Main execution ---\n",
    "\n",
    "def main():\n",
    "    input_file = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Market.xlsx\"\n",
    "    # Output file name\n",
    "    output_file = \"market_historical_weather_daily.xlsx\"\n",
    "    \n",
    "    df_markets = parse_market_file(input_file)\n",
    "    \n",
    "    if df_markets is not None:\n",
    "        df_weather = fetch_weather_for_markets(df_markets)\n",
    "        \n",
    "        if df_weather is not None:\n",
    "            # We have one big DataFrame. We can still save it to one\n",
    "            # Excel file, but saving each market to its own sheet\n",
    "            # is still a good idea for readability.\n",
    "            \n",
    "            print(f\"\\nWriting all market data to Excel file: {output_file}\")\n",
    "            \n",
    "            try:\n",
    "                with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                    # Group the big DataFrame by market name\n",
    "                    grouped = df_weather.groupby('market_name_lookup')\n",
    "                    \n",
    "                    for market_name, df_market_data in grouped:\n",
    "                        # Clean sheet name (Excel limits to 31 chars, no special chars)\n",
    "                        safe_sheet_name = re.sub(r'[\\/:*?\\[\\]]', '', market_name)[:31]\n",
    "                        \n",
    "                        # --- Re-order columns for better readability ---\n",
    "                        # We define the desired order here\n",
    "                        cols_to_order = [\n",
    "                            'market_name_lookup', \n",
    "                            'latitude_lookup', \n",
    "                            'longitude_lookup', \n",
    "                            'time', \n",
    "                            'temperature_2m_max', \n",
    "                            'temperature_2m_min', \n",
    "                            'temperature_2m_mean',\n",
    "                            'precipitation_sum', \n",
    "                            'rain_sum', \n",
    "                            'snowfall_sum',\n",
    "                            'wind_speed_10m_max', \n",
    "                            'wind_gusts_10m_max',\n",
    "                            'weather_code',\n",
    "                            'cloud_cover_mean',\n",
    "                            'uv_index_max'\n",
    "                        ]\n",
    "                        \n",
    "                        # Get a list of columns that actually exist in the dataframe\n",
    "                        # in the desired order\n",
    "                        existing_cols_in_order = [c for c in cols_to_order if c in df_market_data.columns]\n",
    "                        \n",
    "                        # Get any other columns that might exist but aren't in our list\n",
    "                        # (so we don't lose data)\n",
    "                        other_cols = [c for c in df_market_data.columns if c not in existing_cols_in_order]\n",
    "                        \n",
    "                        # Final column list\n",
    "                        final_cols = existing_cols_in_order + other_cols\n",
    "                        \n",
    "                        print(f\"  > Writing sheet: {safe_sheet_name}...\")\n",
    "                        # Save to excel with the new column order\n",
    "                        df_market_data[final_cols].to_excel(writer, sheet_name=safe_sheet_name, index=False)\n",
    "                \n",
    "                print(f\"\\nSuccessfully collected all data and saved to '{output_file}'.\")\n",
    "            \n",
    "            except ImportError:\n",
    "                print(\"\\nError: 'openpyxl' is required to write Excel files.\")\n",
    "                print(\"Please install it: pip install openpyxl\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\nAn error occurred while writing the Excel file: {e}\")\n",
    "                \n",
    "        else:\n",
    "            print(\"Failed to fetch any weather data.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2db6303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Bardoli.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Bardoli(Kat).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Bardoli(Ma).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Kosamba.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Kosamba(D).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Mahuva.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Mahuva(Am).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Mandvi.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Nizar.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Nizar(Kuka).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Nizar(Pumb).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\S.Mandvi.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Songadh.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Songadh(B).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Songadh(U).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Surat.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Uchhal.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Valod.csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Valod(Buh).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Vyara(Pan).csv\n",
      "Exported: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\\Vyra.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the path to the Excel file\n",
    "excel_path = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\market_historical_weather_daily.xlsx\"\n",
    "\n",
    "# Define the output directory for CSV files\n",
    "output_dir = os.path.join(os.path.dirname(excel_path), \"csv_exports\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = pd.ExcelFile(excel_path)\n",
    "\n",
    "# Loop through each sheet and export to CSV\n",
    "for sheet_name in excel_file.sheet_names:\n",
    "    df = excel_file.parse(sheet_name)\n",
    "    csv_filename = f\"{sheet_name}.csv\"\n",
    "    csv_path = os.path.join(output_dir, csv_filename)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Exported: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8d09c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tables created successfully.\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Connect to your MySQL server\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",         # or your server IP\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"weather_history\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# List of cleaned market names\n",
    "market_names = [\n",
    "    \"Bardoli_Kar\", \"Bardoli_Men\", \"Kosamba\", \"Kosamba_D\", \"Mahuva\",\n",
    "    \"Mahuva_Am\", \"Mandvi\", \"Nizar\", \"Nizar_Kuka\", \"Nizar_Purn\", \"S_Mandvi\",\n",
    "    \"Songadh\", \"Songadh_B\", \"Songadh_U\", \"Surat\", \"Uchhal\", \"Valod\",\n",
    "    \"Valod_Buh\", \"Vyara_Pan\", \"Vyra\"\n",
    "]\n",
    "\n",
    "# SQL template\n",
    "table_schema = \"\"\"\n",
    "DROP TABLE IF EXISTS {table_name};\n",
    "\n",
    "CREATE TABLE {table_name} (\n",
    "    market_name VARCHAR(100),\n",
    "    latitude DECIMAL(8,5),\n",
    "    longitude DECIMAL(8,5),\n",
    "    time DATE,\n",
    "    temperature_max DECIMAL(5,2),\n",
    "    temperature_min DECIMAL(5,2),\n",
    "    temperature_mean DECIMAL(5,2),\n",
    "    precipitation_sum DECIMAL(6,2),\n",
    "    rain_sum DECIMAL(6,2),\n",
    "    wind_speed_max DECIMAL(5,2),\n",
    "    wind_gusts_max DECIMAL(5,2),\n",
    "    weather_code INT,\n",
    "    cloud_cover DECIMAL(5,2)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# Execute SQL for each table\n",
    "for name in market_names:\n",
    "    clean_name = name.replace(\"(\", \"\").replace(\")\", \"\").replace(\".\", \"\").replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    sql = table_schema.format(table_name=clean_name)\n",
    "    for statement in sql.strip().split(\";\"):\n",
    "        if statement.strip():\n",
    "            cursor.execute(statement)\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"All tables created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba3942",
   "metadata": {},
   "source": [
    "Load CSVs into MySQL Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d1152c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ï¸ Skipping Bardoli: already contains 367 rows.\n",
      "â­ï¸ Skipping Bardoli_Kar: already contains 367 rows.\n",
      "â­ï¸ Skipping Bardoli_Ma: already contains 367 rows.\n",
      "â­ï¸ Skipping Kosamba: already contains 367 rows.\n",
      "â­ï¸ Skipping Kosamba_D: already contains 367 rows.\n",
      "â­ï¸ Skipping Mahuva: already contains 367 rows.\n",
      "â­ï¸ Skipping Mahuva_Am: already contains 367 rows.\n",
      "â­ï¸ Skipping Mandvi: already contains 367 rows.\n",
      "â­ï¸ Skipping Nizar: already contains 367 rows.\n",
      "â­ï¸ Skipping Nizar_Kuka: already contains 367 rows.\n",
      "â­ï¸ Skipping Nizar_Pumb: already contains 367 rows.\n",
      "â­ï¸ Skipping S_Mandvi: already contains 367 rows.\n",
      "â­ï¸ Skipping Songadh: already contains 367 rows.\n",
      "â­ï¸ Skipping Songadh_B: already contains 367 rows.\n",
      "â­ï¸ Skipping Songadh_U: already contains 367 rows.\n",
      "â­ï¸ Skipping Surat: already contains 367 rows.\n",
      "â­ï¸ Skipping Uchhal: already contains 367 rows.\n",
      "â­ï¸ Skipping Valod: already contains 367 rows.\n",
      "â­ï¸ Skipping Valod_Buh: already contains 367 rows.\n",
      "â­ï¸ Skipping Vyara_Pan: already contains 367 rows.\n",
      "â­ï¸ Skipping Vyra: already contains 367 rows.\n",
      "ðŸŽ¯ All CSVs processed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Connect to MySQL\n",
    "conn = mysql.connector.connect(\n",
    "    host=\"localhost\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"weather_history\"\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Folder containing CSV files\n",
    "csv_folder = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\csv_exports\"\n",
    "\n",
    "# Expected column mapping (CSV â†’ SQL)\n",
    "column_map = {\n",
    "    \"market_name_lookup\": \"market_name\",\n",
    "    \"latitude_lookup\": \"latitude\",\n",
    "    \"longitude_lookup\": \"longitude\",\n",
    "    \"temperature_2m_max\": \"temperature_max\",\n",
    "    \"temperature_2m_min\": \"temperature_min\",\n",
    "    \"temperature_2m_mean\": \"temperature_mean\",\n",
    "    \"precipitation_sum\": \"precipitation_sum\",\n",
    "    \"rain_sum\": \"rain_sum\",\n",
    "    \"wind_speed_10m_max\": \"wind_speed_max\",\n",
    "    \"wind_gusts_10m_max\": \"wind_gusts_max\",\n",
    "    \"weather_code\": \"weather_code\",\n",
    "    \"cloud_cover_mean\": \"cloud_cover\",\n",
    "    \"time\": \"time\"\n",
    "}\n",
    "\n",
    "# Table names\n",
    "market_names = [\n",
    "    \"Bardoli\", \"Bardoli_Kar\", \"Bardoli_Ma\", \"Kosamba\", \"Kosamba_D\", \"Mahuva\",\n",
    "    \"Mahuva_Am\", \"Mandvi\", \"Nizar\", \"Nizar_Kuka\", \"Nizar_Pumb\", \"S_Mandvi\",\n",
    "    \"Songadh\", \"Songadh_B\", \"Songadh_U\", \"Surat\", \"Uchhal\", \"Valod\",\n",
    "    \"Valod_Buh\", \"Vyara_Pan\", \"Vyra\"\n",
    "]\n",
    "\n",
    "# Mapping for mismatched CSV filenames\n",
    "csv_name_map = {\n",
    "    \"Bardoli_Kar\": \"Bardoli(Kat)\",\n",
    "    \"Nizar_Kuka\": \"Nizar(Kuka)\",\n",
    "    \"S_Mandvi\": \"S.Mandvi\",\n",
    "    \"Songadh_B\": \"Songadh(B)\",\n",
    "    \"Songadh_U\": \"Songadh(U)\",\n",
    "    \"Bardoli_Ma\": \"Bardoli(Ma)\",\n",
    "    \"Kosamba_D\": \"Kosamba(D)\",\n",
    "    \"Mahuva_Am\": \"Mahuva(Am)\",\n",
    "    \"Nizar_Pumb\": \"Nizar(Pumb)\",\n",
    "    \"Valod_Buh\": \"Valod(Buh)\",\n",
    "    \"Vyara_Pan\": \"Vyara(Pan)\"\n",
    "\n",
    "}\n",
    "\n",
    "# Load and insert each CSV\n",
    "for table_name in market_names:\n",
    "    # Determine CSV filename\n",
    "    csv_filename = csv_name_map.get(table_name, table_name) + \".csv\"\n",
    "    csv_path = os.path.join(csv_folder, csv_filename)\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"âŒ CSV not found: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    # Check if table already has data\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table_name}\")\n",
    "    row_count = cursor.fetchone()[0]\n",
    "    if row_count > 0:\n",
    "        print(f\"â­ï¸ Skipping {table_name}: already contains {row_count} rows.\")\n",
    "        continue\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Rename columns to match SQL schema\n",
    "    df.rename(columns=column_map, inplace=True)\n",
    "\n",
    "    # Filter only expected columns\n",
    "    expected_cols = list(column_map.values())\n",
    "    df = df[[col for col in expected_cols if col in df.columns]]\n",
    "\n",
    "    # Prepare insert query\n",
    "    placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
    "    columns_str = \", \".join(df.columns)\n",
    "    insert_query = f\"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})\"\n",
    "\n",
    "    # Insert rows\n",
    "    for row in df.itertuples(index=False):\n",
    "        cursor.execute(insert_query, tuple(row))\n",
    "\n",
    "    print(f\"âœ… Inserted data into table: {table_name}\")\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"ðŸŽ¯ All CSVs processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763f4bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted 14965 records between 2024-01-01 and 2025-01-01\n",
      "Saved to: D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Agmarknet_Price_Report_2024.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to your CSV file\n",
    "csv_path = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Agmarknet_Price_Report.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure the date column is parsed correctly\n",
    "# Replace 'Arrival_Date' with the actual column name in your CSV that contains dates\n",
    "df['Arrival_Date'] = pd.to_datetime(df['Arrival_Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "# Define date range\n",
    "start_date = pd.to_datetime(\"2024-01-01\")\n",
    "end_date = pd.to_datetime(\"2025-01-01\")\n",
    "\n",
    "# Filter rows within the date range\n",
    "filtered_df = df[(df['Arrival_Date'] >= start_date) & (df['Arrival_Date'] <= end_date)]\n",
    "\n",
    "# Save the filtered data to a new CSV\n",
    "output_path = r\"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Agmarknet_Price_Report_2024.csv\"\n",
    "filtered_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Extracted {len(filtered_df)} records between {start_date.date()} and {end_date.date()}\")\n",
    "print(f\"Saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e78dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AgriCast (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
