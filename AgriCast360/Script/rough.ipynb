{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2909a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mandi Data api Key = 579b464db66ec23bdd000001e3a48235641a458b502501e95c36f78e\n",
    "\n",
    "Mandi Data file = \"D:\\CUDA_Experiments\\Git_HUB\\AgriCast360\\Script\\Mandi_Data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9945838",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weatherbit API\n",
    "\n",
    "Example API URL: https://api.weatherbit.io/v2.0/history/subhourly?lat=35.78&lon=78.64&start_date=2025-11-11&end_date=2025-11-12&key=API_KEY\n",
    "\n",
    "Master API Key = de8e274a65ec45929d501a63466da6cf\t\n",
    "\n",
    "(Plan\tDetails\tSupport Level\n",
    "Business Trial (Expires 2025-12-03)\n",
    "1,500 req/day\n",
    "1,500 historical req/day\n",
    "25 years historical\n",
    "Current weather + alerts + lightning\n",
    "Daily forecasts\n",
    "Hourly forecasts\n",
    "60 minute forecasts\n",
    "+ Energy / Air Quality / Agweather / Climate Normals API\n",
    "Non-Commercial use only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea1328c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Weatherbit Batch Scraper ---\n",
      "Found 33 markets to process.\n",
      "Loading progress from scrape_progress.json...\n",
      "Market Bardoli is already 'completed'. Skipping.\n",
      "Market Bardoli_Katod is already 'completed'. Skipping.\n",
      "Market Bardoli_Madhi is already 'completed'. Skipping.\n",
      "Market Kosamba is already 'completed'. Skipping.\n",
      "Market Kosamba_Vankal is already 'completed'. Skipping.\n",
      "Market Kosamba_Zangvav is already 'completed'. Skipping.\n",
      "Market Mahuva is already 'completed'. Skipping.\n",
      "Market Mahuva_Anaval is already 'completed'. Skipping.\n",
      "Market Mandvi is already 'completed'. Skipping.\n",
      "Market Nizar is already 'completed'. Skipping.\n",
      "Market Nizar_Kukarmuda is already 'completed'. Skipping.\n",
      "Market Nizar_Pumkitalov is already 'completed'. Skipping.\n",
      "\n",
      "--- Processing Market: Songadh ---\n",
      "File Songadh_weather_2024-01-01_to_2024-12-31.csv already exists. New data will be appended.\n",
      "Resuming from 2024-08-02...\n",
      "Total days to query for this market: 152.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Songadh [2024-08-02]:   0%|          | 0/152 [00:00<?, ?it/s, Key: ...92e0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Key ...92e0 failed (Rate Limit/Auth). Trying next key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Songadh [2024-08-02]:   0%|          | 0/152 [00:03<?, ?it/s, Key: ...f15c]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Key ...f15c failed (Rate Limit/Auth). Trying next key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Songadh [2024-08-02]:   0%|          | 0/152 [00:07<?, ?it/s, Key: ...6e9f]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Key ...6e9f failed (Rate Limit/Auth). Trying next key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Songadh [2024-08-02]:   0%|          | 0/152 [00:10<?, ?it/s, Key: ...3028]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Key ...3028 failed (Rate Limit/Auth). Trying next key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Songadh [2024-08-02]:   0%|          | 0/152 [00:13<?, ?it/s, Key: ...fa54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Key ...fa54 failed (Rate Limit/Auth). Trying next key.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching for Songadh [2024-08-02]:   0%|          | 0/152 [00:19<?, ?it/s, Key: ...fa54]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CRITICAL: All API keys are exhausted. Stopping script.\n",
      "Saving partial data before stopping...\n",
      "\n",
      "No new data to save in this batch.\n",
      "Partial data saved. Progress will be updated.\n",
      "Run the script again tomorrow (or add new keys) to resume.\n",
      "\n",
      "Batch process stopped.\n",
      "Run the script again later to resume from the last checkpoint.\n",
      "--- Batch processing finished. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1. ADD YOUR API KEYS HERE\n",
    "# It will rotate through this list if one key hits its rate limit.\n",
    "API_KEYS = [\n",
    "    \"de8e274a65ec45929d501a63466da6cf\", # Business Trial 1 (Expires 2025-12-03)\n",
    "    \"7335451fcc0646bb86611027b68292e0\", # Business Trial 2 (Expires 2025-12-07)\n",
    "    \"9e56a4f3fab748418bc299713138f15c\", # Business Trial 3 (Expires 2025-12-07)\n",
    "    \"fa8e8af08314f64ab0d2f7492b46e9f\", # Business Trial 4 (Expires 2025-12-07)\n",
    "    \"680cfd56aefc45d6bfa1d40fb65a3028\", # Business Trial 5 (Expires 2025-12-08)\n",
    "    \"a1d90659369044278f0d29c1497dfa54\"  # NEW KEY (Provisioning)\n",
    "]\n",
    "# Global index to track which key we are currently using\n",
    "CURRENT_KEY_INDEX = 1\n",
    "\n",
    "# 2. ADD YOUR MARKETS HERE\n",
    "# The script will loop through these and save a CSV for each.\n",
    "MARKETS = {\n",
    "    \"Bardoli\": {\"lat\": 21.1439076246341, \"lon\": 73.249972681491997},\n",
    "    \"Bardoli_Katod\": {\"lat\": 21.12, \"lon\": 73.12},\n",
    "    \"Bardoli_Madhi\": {\"lat\": 21.15, \"lon\": 73.25},\n",
    "    \"Kosamba\": {\"lat\": 21.464459923454399, \"lon\": 72.952089010333296},\n",
    "    \"Kosamba_Vankal\": {\"lat\": 21.43, \"lon\": 73.23},\n",
    "    \"Kosamba_Zangvav\": {\"lat\": 21.48, \"lon\": 72.95},\n",
    "    \"Mahuva\": {\"lat\": 21.097129557468101, \"lon\": 71.760557527893894},\n",
    "    \"Mahuva_Anaval\": {\"lat\": 20.84, \"lon\": 73.26},\n",
    "    \"Mandvi\": {\"lat\": 21.259469954916099, \"lon\": 73.306064794408599},\n",
    "    \"Nizar\": {\"lat\": 21.47, \"lon\": 74.19},\n",
    "    \"Nizar_Kukarmuda\": {\"lat\": 21.51, \"lon\": 74.13},\n",
    "    \"Nizar_Pumkitalov\": {\"lat\": 21.47, \"lon\": 74.10},\n",
    "    \"Songadh\": {\"lat\": 21.175868577082898, \"lon\": 73.564033008192197},\n",
    "    \"Songadh_Badarpada\": {\"lat\": 21.16, \"lon\": 73.56},\n",
    "    \"Songadh_Umrada\": {\"lat\": 21.16, \"lon\": 73.56},\n",
    "    \"Surat\": {\"lat\": 21.193417012914299, \"lon\": 72.852982768754401},\n",
    "    \"Uchhal\": {\"lat\": 21.17, \"lon\": 73.74},\n",
    "    \"Valod_Buhari\": {\"lat\": 20.97, \"lon\": 73.31},\n",
    "    \"Vyara_Paati\": {\"lat\": 21.11, \"lon\": 73.38},\n",
    "    \"Vyra\": {\"lat\": 21.112412216859902, \"lon\": 73.388557572057493},\n",
    "    \"Amreli\": {\"lat\": 21.559338614206901, \"lon\": 71.227430257825006},\n",
    "    \"Babra\": {\"lat\": 21.848355060711398, \"lon\": 71.311297950245503},\n",
    "    \"Bagasara\": {\"lat\": 21.496945117777699, \"lon\": 70.959329215009802},\n",
    "    \"Dhari\": {\"lat\": 21.331365216974699, \"lon\": 71.023828198856805},\n",
    "    \"Rajula\": {\"lat\": 21.0339169150099, \"lon\": 71.443913765569206},\n",
    "    \"Savarkundla\": {\"lat\": 21.332621199094, \"lon\": 71.313830460040407},\n",
    "    \"Ahmedabad_Chimanbhai_Patal_Market_Vasana\": {\"lat\": 22.996976855033001, \"lon\": 72.536392460074595},\n",
    "    \"Bavla\": {\"lat\": 22.830965521422801, \"lon\": 72.3579378735662},\n",
    "    \"Dhandhuka\": {\"lat\": 22.377117706820101, \"lon\": 71.978523283421794},\n",
    "    \"Dholka\": {\"lat\": 22.7212505077608, \"lon\": 72.448905310358995},\n",
    "    \"Mandal\": {\"lat\": 23.282162015159599, \"lon\": 71.915345532018506},\n",
    "    \"Sanad\": {\"lat\": 22.997609695821499, \"lon\": 72.381634273951903},\n",
    "    \"Viramgam\": {\"lat\": 23.125945592319901, \"lon\": 72.045712191155303}\n",
    "}\n",
    "\n",
    "# 3. SETTINGS\n",
    "BASE_URL = \"https://api.weatherbit.io/v2.0/history/daily\"\n",
    "START_DATE = '2024-01-01'\n",
    "END_DATE = '2024-12-31'\n",
    "\n",
    "# Columns to remove from the final CSV\n",
    "COLUMNS_TO_DROP = ['snow_rate', 'weather.icon', 'revision_version', 'timestamp_utc']\n",
    "\n",
    "# File to track progress\n",
    "PROGRESS_FILE = 'scrape_progress.json'\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "def get_api_key():\n",
    "    \"\"\"\n",
    "    Gets the current API key and advances the index for rotation.\n",
    "    Returns None if all keys are exhausted.\n",
    "    \"\"\"\n",
    "    global CURRENT_KEY_INDEX\n",
    "    if CURRENT_KEY_INDEX >= len(API_KEYS):\n",
    "        return None  # All keys have been tried and failed\n",
    "    \n",
    "    key = API_KEYS[CURRENT_KEY_INDEX]\n",
    "    return key\n",
    "\n",
    "def rotate_to_next_key():\n",
    "    \"\"\"\n",
    "    Moves to the next key in the list.\n",
    "    Returns True if a new key is available, False if all keys are exhausted.\n",
    "    \"\"\"\n",
    "    global CURRENT_KEY_INDEX\n",
    "    CURRENT_KEY_INDEX += 1\n",
    "    if CURRENT_KEY_INDEX >= len(API_KEYS):\n",
    "        return False  # No more keys to try\n",
    "    return True\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Loads the progress file if it exists.\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        print(f\"Loading progress from {PROGRESS_FILE}...\")\n",
    "        try:\n",
    "            with open(PROGRESS_FILE, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Warning: Progress file is corrupt. Starting from scratch.\")\n",
    "            return {}\n",
    "    print(\"No progress file found, starting from scratch.\")\n",
    "    return {}\n",
    "\n",
    "def save_progress(progress):\n",
    "    \"\"\"Saves the current progress to the checkpoint file.\"\"\"\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump(progress, f, indent=4)\n",
    "\n",
    "def parse_date(date_str):\n",
    "    \"\"\"Converts YYYY-MM-DD string to datetime object.\"\"\"\n",
    "    return pd.to_datetime(date_str)\n",
    "\n",
    "def format_date(date_obj):\n",
    "    \"\"\"Converts datetime object to YYYY-MM-DD string.\"\"\"\n",
    "    return date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "def save_data_to_csv(new_data_frames, output_filename):\n",
    "    \"\"\"\n",
    "    Appends new data to the CSV file, handling duplicates and headers.\n",
    "    \"\"\"\n",
    "    if not new_data_frames:\n",
    "        print(\"\\nNo new data to save in this batch.\")\n",
    "        return 'SUCCESS'\n",
    "\n",
    "    print(f\"\\nSaving {len(new_data_frames)} new records to CSV...\")\n",
    "    \n",
    "    try:\n",
    "        new_df = pd.concat(new_data_frames, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error concatenating new dataframes: {e}\")\n",
    "        return 'SUCCESS' # Nothing to save\n",
    "\n",
    "    combined_df = new_df\n",
    "\n",
    "    # Load existing data (if any) and combine\n",
    "    if os.path.exists(output_filename):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(output_filename)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            print(f\"Loaded {len(existing_df)} existing records. Total records before de-dupe: {len(combined_df)}\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Existing file {output_filename} is empty. Using new data.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {output_filename}: {e}. Overwriting with new data.\")\n",
    "    else:\n",
    "        print(\"Creating new file...\")\n",
    "\n",
    "    # De-duplicate, keeping the last (newest) record for any given day\n",
    "    combined_df.drop_duplicates(subset=['market_name', 'datetime'], keep='last', inplace=True)\n",
    "    \n",
    "    # Drop unwanted columns\n",
    "    existing_cols_to_drop = [col for col in COLUMNS_TO_DROP if col in combined_df.columns]\n",
    "    if existing_cols_to_drop:\n",
    "        combined_df.drop(columns=existing_cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Save to CSV\n",
    "    try:\n",
    "        combined_df.to_csv(output_filename, index=False)\n",
    "        print(f\"Successfully saved {len(combined_df)} total unique records to {output_filename}\")\n",
    "        return 'SUCCESS'\n",
    "    except PermissionError:\n",
    "        print(f\"\\nCRITICAL: Could not save {output_filename}. Is the file open in Excel?\")\n",
    "        print(\"Please close the file and re-run the script.\")\n",
    "        return 'STOP_FROM_SAVE'\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCRITICAL: Failed to save CSV: {e}\")\n",
    "        return 'STOP_FROM_SAVE'\n",
    "\n",
    "\n",
    "def fetch_weather_data(market_name, lat, lon, start_date, end_date, progress):\n",
    "    \"\"\"\n",
    "    Fetches weather data for a specific market and date range.\n",
    "    Handles API key rotation and rate limiting.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use pandas to create the list of days to query\n",
    "    all_days = pd.date_range(start_date, end_date, freq='D')\n",
    "    total_days = len(all_days)\n",
    "    \n",
    "    # Get filename for this market's CSV\n",
    "    output_filename = f\"{market_name.replace(' ', '_')}_weather_{start_date}_to_{end_date}.csv\"\n",
    "    \n",
    "    # This list will ONLY hold NEW data fetched in this run\n",
    "    new_data_fetched_this_run = [] \n",
    "    if os.path.exists(output_filename):\n",
    "        print(f\"File {output_filename} already exists. New data will be appended.\")\n",
    "    else:\n",
    "        print(f\"Creating new file: {output_filename}\")\n",
    "\n",
    "    # --- Progress Loading Logic ---\n",
    "    market_progress_data = progress.get(market_name, {})\n",
    "    last_fetched_day_str = None\n",
    "    market_status = None\n",
    "\n",
    "    if isinstance(market_progress_data, dict):\n",
    "        last_fetched_day_str = market_progress_data.get('last_fetched_day')\n",
    "        market_status = market_progress_data.get('status')\n",
    "    elif isinstance(market_progress_data, str):\n",
    "        if market_progress_data == 'completed':\n",
    "            market_status = 'completed'\n",
    "            last_fetched_day_str = None\n",
    "        else:\n",
    "            last_fetched_day_str = market_progress_data\n",
    "            print(\"Detected old progress format. Will update to new format.\")\n",
    "    \n",
    "    if market_status == 'completed':\n",
    "        print(f\"Market {market_name} is already marked as 'completed'. Skipping.\")\n",
    "        return 'CONTINUE'\n",
    "\n",
    "    # Determine where to start fetching from\n",
    "    if last_fetched_day_str:\n",
    "        start_fetching_from = parse_date(last_fetched_day_str) + timedelta(days=1)\n",
    "        days_to_query = [day for day in all_days if day >= start_fetching_from]\n",
    "        if start_fetching_from > parse_date(end_date):\n",
    "            print(\"All days already fetched. Marking as complete.\")\n",
    "            days_to_query = []\n",
    "        else:\n",
    "            print(f\"Resuming from {format_date(start_fetching_from)}...\")\n",
    "    else:\n",
    "        days_to_query = all_days\n",
    "        print(f\"Starting from {start_date}...\")\n",
    "\n",
    "    # FIX 1: Handle ValueError for DatetimeIndex\n",
    "    if len(days_to_query) == 0:\n",
    "        print(\"No new days to query. Marking as complete.\")\n",
    "        progress[market_name] = {'status': 'completed', 'last_fetched_day': end_date}\n",
    "        save_progress(progress)\n",
    "        return 'CONTINUE'\n",
    "\n",
    "    print(f\"Total days to query for this market: {len(days_to_query)}.\")\n",
    "\n",
    "    # --- MAIN FETCHING LOOP (RESTRUCTURED) ---\n",
    "    with tqdm(total=len(days_to_query), desc=f\"Fetching for {market_name}\") as pbar:\n",
    "        for day in days_to_query:\n",
    "            \n",
    "            # This inner loop will retry the *same day* until it\n",
    "            # succeeds or all keys are exhausted.\n",
    "            day_fetched_successfully = False\n",
    "            \n",
    "            # FIX 2: Added 'while' loop for retries\n",
    "            while not day_fetched_successfully:\n",
    "                \n",
    "                api_key = get_api_key()\n",
    "                if api_key is None:\n",
    "                    # All keys are exhausted for today.\n",
    "                    print(\"\\nCRITICAL: All API keys are exhausted. Stopping script.\")\n",
    "                    print(\"Saving partial data before stopping...\")\n",
    "                    save_status = save_data_to_csv(new_data_fetched_this_run, output_filename)\n",
    "                    if save_status == 'STOP_FROM_SAVE':\n",
    "                        print(\"Could not save partial data. Data will be re-fetched on next run.\")\n",
    "                    else:\n",
    "                        print(\"Partial data saved. Progress will be updated.\")\n",
    "                        save_progress(progress) \n",
    "                    \n",
    "                    print(\"Run the script again tomorrow (or add new keys) to resume.\")\n",
    "                    return 'STOP' # Stop the entire batch process\n",
    "                \n",
    "                pbar.set_description(f\"Fetching for {market_name} [{day.strftime('%Y-%m-%d')}]\", refresh=True)\n",
    "                pbar.set_postfix_str(f\"Key: ...{api_key[-4:]}\")\n",
    "\n",
    "                day_start_str = format_date(day)\n",
    "                day_end_str = format_date(day + timedelta(days=1))\n",
    "\n",
    "                params = {\n",
    "                    'key': api_key,\n",
    "                    'lat': lat,\n",
    "                    'lon': lon,\n",
    "                    'start_date': day_start_str,\n",
    "                    'end_date': day_end_str\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    response = requests.get(BASE_URL, params=params, timeout=10)\n",
    "\n",
    "                    # --- Rate Limit / Auth Error ---\n",
    "                    # This now correctly handles 403 (Forbidden),\n",
    "                    # which your new provisioning key might return.\n",
    "                    if response.status_code == 429 or response.status_code == 403:\n",
    "                        print(f\"\\nWarning: Key ...{api_key[-4:]} failed (Rate Limit/Auth). Trying next key.\")\n",
    "                        if not rotate_to_next_key():\n",
    "                            # This was the last key. get_api_key() will return None\n",
    "                            # on the next 'while' loop iteration, triggering the STOP.\n",
    "                            pass\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "                        # 'continue' will retry the 'while' loop for the SAME DAY\n",
    "                        continue \n",
    "\n",
    "                    # --- Other HTTP Error ---\n",
    "                    if response.status_code != 200:\n",
    "                        print(f\"\\nError: Received status code {response.status_code} for {day_start_str}.\")\n",
    "                        print(f\"Response: {response.text}\")\n",
    "                        time.sleep(1)\n",
    "                        # Break the 'while' loop and skip to the next day.\n",
    "                        # This day will be retried on the next script run.\n",
    "                        break \n",
    "\n",
    "                    # --- Success ---\n",
    "                    data = response.json()\n",
    "                    daily_data = data.get('data', [])\n",
    "                    \n",
    "                    if daily_data:\n",
    "                        df = pd.json_normalize(daily_data)\n",
    "                        df['market_name'] = market_name\n",
    "                        df['query_lat'] = lat\n",
    "                        df['query_lon'] = lon\n",
    "                        new_data_fetched_this_run.append(df)\n",
    "                    \n",
    "                    progress[market_name] = {'last_fetched_day': day_start_str}\n",
    "                    save_progress(progress)\n",
    "                    \n",
    "                    day_fetched_successfully = True # This breaks the 'while' loop\n",
    "                    pbar.update(1) # Advance progress bar by one day\n",
    "                    time.sleep(0.1) # Be nice to the API\n",
    "\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"\\nNetwork Error: {e}. Pausing for 10 seconds...\")\n",
    "                    time.sleep(10)\n",
    "                    # 'continue' will retry the 'while' loop for the SAME DAY\n",
    "                    continue\n",
    "            \n",
    "            # --- End of while loop ---\n",
    "            # The 'for' loop will now proceed to the next day.\n",
    "\n",
    "    # --- Market Finished ---\n",
    "    save_status = save_data_to_csv(new_data_fetched_this_run, output_filename)\n",
    "    if save_status == 'STOP_FROM_SAVE':\n",
    "        print(f\"Could not save data for {market_name}. Market will NOT be marked as complete.\")\n",
    "        return 'STOP' \n",
    "\n",
    "    progress[market_name] = {'status': 'completed', 'last_fetched_day': end_date}\n",
    "    save_progress(progress)\n",
    "    print(f\"--- Finished processing {market_name} ---\")\n",
    "    return 'CONTINUE'\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the batch scraper.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Weatherbit Batch Scraper ---\")\n",
    "    \n",
    "    # Check if argparse is being used in an interactive env (like Jupyter)\n",
    "    # This ignores Jupyter's internal args like --f\n",
    "    if 'ipykernel_launcher.py' in sys.argv[0]:\n",
    "        sys.argv = [sys.argv[0]]  # Keep only the script name\n",
    "\n",
    "    # (argparse setup is removed as we are hardcoding markets)\n",
    "\n",
    "    print(f\"Found {len(MARKETS)} markets to process.\")\n",
    "\n",
    "    # Load progress from checkpoint file\n",
    "    progress = load_progress()\n",
    "\n",
    "    # Loop over each market defined in the config\n",
    "    for market_name, coords in MARKETS.items():\n",
    "        \n",
    "        # --- Backwards-Compatibility Check ---\n",
    "        market_progress_data = progress.get(market_name, {})\n",
    "        market_status = None\n",
    "        if isinstance(market_progress_data, dict):\n",
    "            market_status = market_progress_data.get('status')\n",
    "        elif isinstance(market_progress_data, str):\n",
    "            if market_progress_data == 'completed':\n",
    "                market_status = 'completed'\n",
    "            else:\n",
    "                market_status = None \n",
    "                print(f\"Detected old progress format for {market_name}. Will update.\")\n",
    "\n",
    "        if market_status == 'completed':\n",
    "            print(f\"Market {market_name} is already 'completed'. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- Processing Market: {market_name} ---\")\n",
    "        \n",
    "        status = fetch_weather_data(\n",
    "            market_name=market_name,\n",
    "            lat=coords['lat'],\n",
    "            lon=coords['lon'],\n",
    "            start_date=START_DATE,\n",
    "            end_date=END_DATE,\n",
    "            progress=progress # Pass the main progress object\n",
    "        )\n",
    "        \n",
    "        if status == 'STOP':\n",
    "            print(\"\\nBatch process stopped.\")\n",
    "            print(\"Run the script again later to resume from the last checkpoint.\")\n",
    "            break # Stop processing other markets\n",
    "    \n",
    "    print(\"--- Batch processing finished. ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8335b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
